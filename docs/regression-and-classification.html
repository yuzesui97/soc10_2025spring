<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Week 3 Regression and Classification | Stanford Spring 2025 Intro to Computational Social Science</title>
  <meta name="description" content="Week 3 Regression and Classification | Stanford Spring 2025 Intro to Computational Social Science" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Week 3 Regression and Classification | Stanford Spring 2025 Intro to Computational Social Science" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Week 3 Regression and Classification | Stanford Spring 2025 Intro to Computational Social Science" />
  
  
  

<meta name="author" content="Yuze Sui" />


<meta name="date" content="2025-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="surveys-and-survey-experiments-with-qualtrics.html"/>
<link rel="next" href="social-network-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro_Comp_Social_Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="chapter" data-level="1" data-path="intro-to-r.html"><a href="intro-to-r.html"><i class="fa fa-check"></i><b>1</b> Intro to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-to-r.html"><a href="intro-to-r.html#r-basics"><i class="fa fa-check"></i><b>1.1</b> R Basics</a></li>
<li class="chapter" data-level="1.2" data-path="intro-to-r.html"><a href="intro-to-r.html#vectors"><i class="fa fa-check"></i><b>1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.3" data-path="intro-to-r.html"><a href="intro-to-r.html#loading-packages"><i class="fa fa-check"></i><b>1.3</b> Loading Packages</a></li>
<li class="chapter" data-level="1.4" data-path="intro-to-r.html"><a href="intro-to-r.html#exploring-and-visualizing-data"><i class="fa fa-check"></i><b>1.4</b> Exploring and Visualizing Data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html"><i class="fa fa-check"></i><b>2</b> Surveys and Survey Experiments with Qualtrics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html#creating-a-qualtrics-account"><i class="fa fa-check"></i><b>2.2</b> Creating a Qualtrics account</a></li>
<li class="chapter" data-level="2.3" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html#survey-options"><i class="fa fa-check"></i><b>2.3</b> Survey options</a></li>
<li class="chapter" data-level="2.4" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html#a-quick-survey-experiment"><i class="fa fa-check"></i><b>2.4</b> A quick survey experiment</a></li>
<li class="chapter" data-level="2.5" data-path="surveys-and-survey-experiments-with-qualtrics.html"><a href="surveys-and-survey-experiments-with-qualtrics.html#publish-it"><i class="fa fa-check"></i><b>2.5</b> Publish it!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression-and-classification.html"><a href="regression-and-classification.html"><i class="fa fa-check"></i><b>3</b> Regression and Classification</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regression-and-classification.html"><a href="regression-and-classification.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear Regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression-and-classification.html"><a href="regression-and-classification.html#how-linear-regression-works"><i class="fa fa-check"></i><b>3.2</b> How linear regression works</a></li>
<li class="chapter" data-level="3.3" data-path="regression-and-classification.html"><a href="regression-and-classification.html#linear-regression-using-the-lm-function"><i class="fa fa-check"></i><b>3.3</b> Linear regression using the lm function</a></li>
<li class="chapter" data-level="3.4" data-path="regression-and-classification.html"><a href="regression-and-classification.html#multiple-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Regression</a></li>
<li class="chapter" data-level="3.5" data-path="regression-and-classification.html"><a href="regression-and-classification.html#linear-regression-prediction"><i class="fa fa-check"></i><b>3.5</b> Linear Regression Prediction</a></li>
<li class="chapter" data-level="3.6" data-path="regression-and-classification.html"><a href="regression-and-classification.html#classification-logistic-regression"><i class="fa fa-check"></i><b>3.6</b> Classification (Logistic Regression)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="social-network-analysis.html"><a href="social-network-analysis.html"><i class="fa fa-check"></i><b>4</b> Social Network Analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="social-network-analysis.html"><a href="social-network-analysis.html#understanding-network-data-structures-in-r"><i class="fa fa-check"></i><b>4.1</b> Understanding network data structures in R</a></li>
<li class="chapter" data-level="4.2" data-path="social-network-analysis.html"><a href="social-network-analysis.html#visualizing-network-data-in-r"><i class="fa fa-check"></i><b>4.2</b> Visualizing network data in R</a></li>
<li class="chapter" data-level="4.3" data-path="social-network-analysis.html"><a href="social-network-analysis.html#key-network-measures"><i class="fa fa-check"></i><b>4.3</b> Key network measures</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="social-network-analysis.html"><a href="social-network-analysis.html#degree-how-many-friends-do-i-have"><i class="fa fa-check"></i><b>4.3.1</b> Degree (“How many friends do I have?”)</a></li>
<li class="chapter" data-level="4.3.2" data-path="social-network-analysis.html"><a href="social-network-analysis.html#weighted-degree-strength"><i class="fa fa-check"></i><b>4.3.2</b> Weighted degree (strength)</a></li>
<li class="chapter" data-level="4.3.3" data-path="social-network-analysis.html"><a href="social-network-analysis.html#global-clustering-coefficient-gcc"><i class="fa fa-check"></i><b>4.3.3</b> Global Clustering Coefficient (GCC)</a></li>
<li class="chapter" data-level="4.3.4" data-path="social-network-analysis.html"><a href="social-network-analysis.html#average-path-length-apl"><i class="fa fa-check"></i><b>4.3.4</b> Average path length (APL)</a></li>
<li class="chapter" data-level="4.3.5" data-path="social-network-analysis.html"><a href="social-network-analysis.html#assortativity"><i class="fa fa-check"></i><b>4.3.5</b> Assortativity</a></li>
<li class="chapter" data-level="4.3.6" data-path="social-network-analysis.html"><a href="social-network-analysis.html#betweenness-centrality"><i class="fa fa-check"></i><b>4.3.6</b> Betweenness centrality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="social-network-analysis.html"><a href="social-network-analysis.html#visualizing-key-measures"><i class="fa fa-check"></i><b>4.4</b> Visualizing Key Measures</a></li>
<li class="chapter" data-level="4.5" data-path="social-network-analysis.html"><a href="social-network-analysis.html#benchmark-our-empirical-network"><i class="fa fa-check"></i><b>4.5</b> Benchmark our empirical network</a></li>
<li class="chapter" data-level="4.6" data-path="social-network-analysis.html"><a href="social-network-analysis.html#find-local-clusterscommunities"><i class="fa fa-check"></i><b>4.6</b> Find local clusters/communities</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="social-network-analysis.html"><a href="social-network-analysis.html#other-resources"><i class="fa fa-check"></i><b>4.6.1</b> Other Resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="collecting-data-online.html"><a href="collecting-data-online.html"><i class="fa fa-check"></i><b>5</b> Collecting Data Online</a>
<ul>
<li class="chapter" data-level="5.1" data-path="collecting-data-online.html"><a href="collecting-data-online.html#scraping-the-web"><i class="fa fa-check"></i><b>5.1</b> Scraping the web</a></li>
<li class="chapter" data-level="5.2" data-path="collecting-data-online.html"><a href="collecting-data-online.html#google-news-api"><i class="fa fa-check"></i><b>5.2</b> Google News API</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="collecting-data-online.html"><a href="collecting-data-online.html#prerequistes"><i class="fa fa-check"></i><b>5.2.1</b> Prerequistes</a></li>
<li class="chapter" data-level="5.2.2" data-path="collecting-data-online.html"><a href="collecting-data-online.html#get-started"><i class="fa fa-check"></i><b>5.2.2</b> Get started</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>6</b> Text Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="text-analysis.html"><a href="text-analysis.html#sentiment-analysis"><i class="fa fa-check"></i><b>6.1</b> Sentiment Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="text-analysis.html"><a href="text-analysis.html#topic-modeling"><i class="fa fa-check"></i><b>6.2</b> Topic Modeling</a></li>
<li class="chapter" data-level="6.3" data-path="text-analysis.html"><a href="text-analysis.html#word-embeddings"><i class="fa fa-check"></i><b>6.3</b> Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html"><i class="fa fa-check"></i><b>7</b> Machine Learning (Supervised Learning)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html#training-vs.-test-data-why-split"><i class="fa fa-check"></i><b>7.1</b> Training vs. Test Data: Why Split?</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html#splitting-the-gss-2018-data"><i class="fa fa-check"></i><b>7.1.1</b> Splitting the GSS 2018 Data</a></li>
<li class="chapter" data-level="7.1.2" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html#linear-regression-with-training-and-test-data"><i class="fa fa-check"></i><b>7.1.2</b> Linear Regression with Training and Test Data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html#decision-trees-for-regression"><i class="fa fa-check"></i><b>7.2</b> Decision Trees for Regression</a></li>
<li class="chapter" data-level="7.3" data-path="machine-learning-supervised-learning.html"><a href="machine-learning-supervised-learning.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="large-language-models.html"><a href="large-language-models.html"><i class="fa fa-check"></i><b>8</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="large-language-models.html"><a href="large-language-models.html#tidychatmodels-package"><i class="fa fa-check"></i><b>8.1</b> tidychatmodels Package</a></li>
<li class="chapter" data-level="8.2" data-path="large-language-models.html"><a href="large-language-models.html#llm-as-synthetic-respondents-for-social-survey"><i class="fa fa-check"></i><b>8.2</b> LLM as “Synthetic Respondents” for Social Survey</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stanford Spring 2025 Intro to Computational Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-and-classification" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Week 3</span> Regression and Classification<a href="regression-and-classification.html#regression-and-classification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This section is drafted based on Dr. Mark Hoffman’s previous SOC 10 lab notes and kassambara’s tutorial on logistic regression with R on STHDA.</em></p>
<p>This week’s lab introduces fundamental machine learning techniques widely used in computational social science to both model and analyze data. In contrast to traditional programming—where explicit instructions are provided at every step—machine learning equips computers with the ability to identify patterns and make decisions based on data. Generally, our objectives in machine learning are threefold: to describe and understand the data, to develop models that test theories about how the data is generated, and to make predictions that generalize to new, similar situations.</p>
<p>We begin by exploring Linear Regression, a popular method in the social sciences for modeling relationships between variables. Regression analysis allows us to quantify how changes in one or more predictor variables are associated with changes in a continuous outcome variable. This technique not only provides insights into the underlying patterns of the data but also is useful for predicting continuous outcomes (e.g., household income, movie box office).</p>
<p>In addition to regression, we will also cover Classification techniques, with a particular focus on Logistic Regression. Unlike regression, which deals with predicting continuous outcomes, classification methods are designed to predict categorical outcomes. For example, logistic regression models the probability of an observation belonging to a particular category—often a binary outcome—by leveraging a logistic function. The key difference between regression and classification lies in their goals: while regression estimates a numerical value, classification assigns an observation to a discrete class based on learned patterns in the data.</p>
<div id="linear-regression" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Linear Regression<a href="regression-and-classification.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous two labs, we learned how to download data and visualize patterns between variables. In what follows, we will go beyond data visualization and begin to ask theoretically informed questions and using data, again, from the GSS, to answer those questions. Where before we plotted two variables against each other to see their relationship, linear regression will allow us to quantify their relationship: how much do changes in our explanatory variable lead to changes in the variable we are hoping to explain? Is this relationship statistically significant - that is, does it differ from what we should expect by chance?</p>
<p>Linear regression will also allow us to adjust for covariates - variables that may be affecting our primary dependent variable of interest as well as our independent variable. For example, in classic example of confounding, we may see that the number of ice cream cones people eat per year is correlated with the number of sunburns they get and think that ice cream causes sunburns. However, it is obvious that both of these factors will be influenced by how warm of a climate people live in - with people living in warmer climates consuming more ice cream AND getting more sunburns. By controlling for the warmth of the climate, we can adjust for this fact and likely any association we saw between ice cream and sunburns will go away.</p>
<p><img src="week3_plot1.png" />
<em>Figure Source: Parsa Ghaffari.</em></p>
</div>
<div id="how-linear-regression-works" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> How linear regression works<a href="regression-and-classification.html#how-linear-regression-works" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may remember the slope-intercept form of writing the equation of a straight line from algebra.</p>
<p><span class="math display">\[
y = mx + b
\]</span></p>
<p>Here, we can calculate the coordinate y for any x by first multiplying x by the slope of the line, m, and adding the value of the intercept, b, which is where the line intersects with the y axis.</p>
<p><img src="week3_plot2.png" />
<em>Figure Source: tecmath.</em></p>
<p>Linear regression is a strategy for finding the line that best fits the relationship between two variables (later on we can add more variables as control variables). We start with a y variable, also called the outcome or the dependent variable, and an x variable, also called a predictor or the independent variable, and ask what is the slope-intercept equation that most closely approximates their relationship. Given x and y, linear regression therefore involves estimating the slope, m, and intercept, b, of the line.</p>
<p>Rarely in real world applications are two variables perfectly related to one another: even the best social science models have error. To reflect this, we update the equation above to:</p>
<p><span class="math display">\[
y = mx + b + ε
\]</span>
With ε capturing the error in our predictions.</p>
<p>How do we fit a line to x and y? The short of it is that we will start with line (say, a horizontal one) and keep adjusting the slope and intercept of that line to minimize the average distance between the data points and the line itself. Residuals are the difference between the observed values and the values predicted by the line:</p>
<p><span class="math display">\[
residual_i=y_i−\hat{y}_i
\]</span></p>
<p>Linear regression will seek to minimize the sum of the squared residuals, also known as the sum of squares:</p>
<p><span class="math display">\[
\text{SumSquares} = \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2
\]</span></p>
<p><img src="week3_plot4.png" />
<em>Figure Source: statisticsfun.</em></p>
</div>
<div id="linear-regression-using-the-lm-function" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Linear regression using the lm function<a href="regression-and-classification.html#linear-regression-using-the-lm-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thankfully, as computational social scientists, we won’t have to do this adjustment by hand. The lm function in R uses an algorithm, called gradient descent, to find the linear regression line that best minimizes the sum of squares for us.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="regression-and-classification.html#cb45-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(realinc <span class="sc">~</span> age, <span class="at">data =</span> gss)</span></code></pre></div>
<p>The first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of real household income as a function of age The second argument specifies that R should look in the gss data frame to find the age and realinc variables.</p>
<p>The output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary function.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="regression-and-classification.html#cb46-1" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="regression-and-classification.html#cb47-1" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb47-2"><a href="regression-and-classification.html#cb47-2" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb47-3"><a href="regression-and-classification.html#cb47-3" tabindex="-1"></a><span class="do">## lm(formula = realinc ~ age, data = gss)</span></span>
<span id="cb47-4"><a href="regression-and-classification.html#cb47-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb47-5"><a href="regression-and-classification.html#cb47-5" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb47-6"><a href="regression-and-classification.html#cb47-6" tabindex="-1"></a><span class="do">##    Min     1Q Median     3Q    Max </span></span>
<span id="cb47-7"><a href="regression-and-classification.html#cb47-7" tabindex="-1"></a><span class="do">## -36142 -21953  -8800  11184  88412 </span></span>
<span id="cb47-8"><a href="regression-and-classification.html#cb47-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb47-9"><a href="regression-and-classification.html#cb47-9" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb47-10"><a href="regression-and-classification.html#cb47-10" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb47-11"><a href="regression-and-classification.html#cb47-11" tabindex="-1"></a><span class="do">## (Intercept) 30130.57    1968.22  15.309   &lt;2e-16 ***</span></span>
<span id="cb47-12"><a href="regression-and-classification.html#cb47-12" tabindex="-1"></a><span class="do">## age            74.27      37.87   1.961     0.05 *  </span></span>
<span id="cb47-13"><a href="regression-and-classification.html#cb47-13" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb47-14"><a href="regression-and-classification.html#cb47-14" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb47-15"><a href="regression-and-classification.html#cb47-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb47-16"><a href="regression-and-classification.html#cb47-16" tabindex="-1"></a><span class="do">## Residual standard error: 31160 on 2145 degrees of freedom</span></span>
<span id="cb47-17"><a href="regression-and-classification.html#cb47-17" tabindex="-1"></a><span class="do">##   (201 observations deleted due to missingness)</span></span>
<span id="cb47-18"><a href="regression-and-classification.html#cb47-18" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.00179,    Adjusted R-squared:  0.001324 </span></span>
<span id="cb47-19"><a href="regression-and-classification.html#cb47-19" tabindex="-1"></a><span class="do">## F-statistic: 3.846 on 1 and 2145 DF,  p-value: 0.04999</span></span></code></pre></div>
<p>Let’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of age. With this table, we can write down the least squares regression line for the linear model:</p>
<p><span class="math display">\[
\hat{y} =30130.57+74.27∗age
\]</span></p>
<p>It also shows whether the coefficients, here, age, have are statistically significant in predicting the outcome, income. Normally, a p-value cut-off of 0.05 is used to determine statistical significance - here, age’s p-value is almost exactly 0.05 (in fact, it is very slightly lower) and is therefore significant.</p>
<p>One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, only %.18 of the variability in income is explained by age.</p>
<p>What variables might do a better job of explaining income? Let’s try years of education, which is the variable educ in the gss. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between income and education</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="regression-and-classification.html#cb48-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(realinc <span class="sc">~</span> educ, <span class="at">data =</span> gss)</span>
<span id="cb48-2"><a href="regression-and-classification.html#cb48-2" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="regression-and-classification.html#cb49-1" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-2"><a href="regression-and-classification.html#cb49-2" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb49-3"><a href="regression-and-classification.html#cb49-3" tabindex="-1"></a><span class="do">## lm(formula = realinc ~ educ, data = gss)</span></span>
<span id="cb49-4"><a href="regression-and-classification.html#cb49-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-5"><a href="regression-and-classification.html#cb49-5" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb49-6"><a href="regression-and-classification.html#cb49-6" tabindex="-1"></a><span class="do">##    Min     1Q Median     3Q    Max </span></span>
<span id="cb49-7"><a href="regression-and-classification.html#cb49-7" tabindex="-1"></a><span class="do">## -57666 -18112  -6093  10864  97362 </span></span>
<span id="cb49-8"><a href="regression-and-classification.html#cb49-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-9"><a href="regression-and-classification.html#cb49-9" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb49-10"><a href="regression-and-classification.html#cb49-10" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb49-11"><a href="regression-and-classification.html#cb49-11" tabindex="-1"></a><span class="do">## (Intercept)   -21552       3006  -7.171 1.02e-12 ***</span></span>
<span id="cb49-12"><a href="regression-and-classification.html#cb49-12" tabindex="-1"></a><span class="do">## educ            4006        213  18.809  &lt; 2e-16 ***</span></span>
<span id="cb49-13"><a href="regression-and-classification.html#cb49-13" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb49-14"><a href="regression-and-classification.html#cb49-14" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb49-15"><a href="regression-and-classification.html#cb49-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-16"><a href="regression-and-classification.html#cb49-16" tabindex="-1"></a><span class="do">## Residual standard error: 28880 on 2149 degrees of freedom</span></span>
<span id="cb49-17"><a href="regression-and-classification.html#cb49-17" tabindex="-1"></a><span class="do">##   (197 observations deleted due to missingness)</span></span>
<span id="cb49-18"><a href="regression-and-classification.html#cb49-18" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.1414, Adjusted R-squared:  0.141 </span></span>
<span id="cb49-19"><a href="regression-and-classification.html#cb49-19" tabindex="-1"></a><span class="do">## F-statistic: 353.8 on 1 and 2149 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p><span class="math display">\[
y=−21552+4006∗educ
\]</span></p>
<p>The higher one’s education, the higher one’s income, generally. Specifically, the slope tells us that for every year of education, a person is expected to see an additional income of 4006 dollars. And this result is statistically significant given the small p-value (this is a typical explanation structure for you to explain linear regression results to readers!)</p>
<p>Further, the intercept tells us that people with 0 years of education are expect to have an income of -21552 dollars. Of course, this is an extrapolation, produced by the linear regression: an income of negative dollars doesn’t make much sense. Finally, we can see from the regression output that the R2 for education is much higher than age: 11%!</p>
<p>Let’s create a scatterplot with the least squares line laid on top.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="regression-and-classification.html#cb50-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb50-2"><a href="regression-and-classification.html#cb50-2" tabindex="-1"></a></span>
<span id="cb50-3"><a href="regression-and-classification.html#cb50-3" tabindex="-1"></a><span class="fu">ggplot</span>(gss, <span class="fu">aes</span>(<span class="at">x =</span> educ, <span class="at">y =</span> realinc)) <span class="sc">+</span></span>
<span id="cb50-4"><a href="regression-and-classification.html#cb50-4" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb50-5"><a href="regression-and-classification.html#cb50-5" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">formula=</span> y<span class="sc">~</span>x)</span></code></pre></div>
<p><img src="week3_plot5.png" /></p>
</div>
<div id="multiple-regression" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Multiple Regression<a href="regression-and-classification.html#multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can build more complex models by adding variables to our model. When we do that, we go from the world of simple linear regression to the more complex world of multiple regression.</p>
<p>Here, for example, is a more complex model with both of the variables we examined above.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="regression-and-classification.html#cb51-1" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(realinc <span class="sc">~</span> age <span class="sc">+</span> educ, <span class="at">data =</span> gss)</span>
<span id="cb51-2"><a href="regression-and-classification.html#cb51-2" tabindex="-1"></a><span class="fu">summary</span>(m3)</span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="regression-and-classification.html#cb52-1" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-2"><a href="regression-and-classification.html#cb52-2" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb52-3"><a href="regression-and-classification.html#cb52-3" tabindex="-1"></a><span class="do">## lm(formula = realinc ~ age + educ, data = gss)</span></span>
<span id="cb52-4"><a href="regression-and-classification.html#cb52-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-5"><a href="regression-and-classification.html#cb52-5" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb52-6"><a href="regression-and-classification.html#cb52-6" tabindex="-1"></a><span class="do">##    Min     1Q Median     3Q    Max </span></span>
<span id="cb52-7"><a href="regression-and-classification.html#cb52-7" tabindex="-1"></a><span class="do">## -60809 -19200  -6908  10063  99951 </span></span>
<span id="cb52-8"><a href="regression-and-classification.html#cb52-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-9"><a href="regression-and-classification.html#cb52-9" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb52-10"><a href="regression-and-classification.html#cb52-10" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb52-11"><a href="regression-and-classification.html#cb52-11" tabindex="-1"></a><span class="do">## (Intercept) -25708.70    3481.35  -7.385 2.18e-13 ***</span></span>
<span id="cb52-12"><a href="regression-and-classification.html#cb52-12" tabindex="-1"></a><span class="do">## age             83.51      35.11   2.378   0.0175 *  </span></span>
<span id="cb52-13"><a href="regression-and-classification.html#cb52-13" tabindex="-1"></a><span class="do">## educ          4012.21     212.96  18.840  &lt; 2e-16 ***</span></span>
<span id="cb52-14"><a href="regression-and-classification.html#cb52-14" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb52-15"><a href="regression-and-classification.html#cb52-15" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb52-16"><a href="regression-and-classification.html#cb52-16" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-17"><a href="regression-and-classification.html#cb52-17" tabindex="-1"></a><span class="do">## Residual standard error: 28870 on 2143 degrees of freedom</span></span>
<span id="cb52-18"><a href="regression-and-classification.html#cb52-18" tabindex="-1"></a><span class="do">##   (202 observations deleted due to missingness)</span></span>
<span id="cb52-19"><a href="regression-and-classification.html#cb52-19" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.1436, Adjusted R-squared:  0.1428 </span></span>
<span id="cb52-20"><a href="regression-and-classification.html#cb52-20" tabindex="-1"></a><span class="do">## F-statistic: 179.7 on 2 and 2143 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>What do we learn? How does the Multiple R-Squared look compard to previous models? Did we improve our model fit? By including both variables, the coefficients now tell us the effect of each variable, conditional on the other. For example, age and education are correlated in complex ways. Younger generations are slightly more educated than older generations. At the same time, being older gives you more time to achieve an education. Controlling for age allows us to interpret education without worrying that age effects of this sort are driving the relationship between education and income. That said, after age is controlled for, while R2 improves, the coefficient size for education stays roughly the same.</p>
<p>Specifically, the slope tells us that controlling for age, for every year of education, a person is expected to see an additional income of 4012.21 dollars. And this result is statistically significant given the small p-value (this is a typical explanation structure for you to explain linear regression results to readers!)</p>
<p>You could also add in categorical variables like sex.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="regression-and-classification.html#cb53-1" tabindex="-1"></a>gss<span class="sc">$</span>sex <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(gss<span class="sc">$</span>sex)</span>
<span id="cb53-2"><a href="regression-and-classification.html#cb53-2" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(realinc <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> sex, <span class="at">data =</span> gss)</span>
<span id="cb53-3"><a href="regression-and-classification.html#cb53-3" tabindex="-1"></a><span class="fu">summary</span>(m4)</span></code></pre></div>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="regression-and-classification.html#cb54-1" tabindex="-1"></a>Call<span class="sc">:</span></span>
<span id="cb54-2"><a href="regression-and-classification.html#cb54-2" tabindex="-1"></a><span class="fu">lm</span>(<span class="at">formula =</span> realinc <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> sex, <span class="at">data =</span> gss)</span>
<span id="cb54-3"><a href="regression-and-classification.html#cb54-3" tabindex="-1"></a></span>
<span id="cb54-4"><a href="regression-and-classification.html#cb54-4" tabindex="-1"></a>Residuals<span class="sc">:</span></span>
<span id="cb54-5"><a href="regression-and-classification.html#cb54-5" tabindex="-1"></a>   Min     <span class="dv">1</span>Q Median     <span class="dv">3</span>Q    Max </span>
<span id="cb54-6"><a href="regression-and-classification.html#cb54-6" tabindex="-1"></a><span class="sc">-</span><span class="dv">58696</span> <span class="sc">-</span><span class="dv">18682</span>  <span class="sc">-</span><span class="dv">6924</span>   <span class="dv">9400</span> <span class="dv">101888</span> </span>
<span id="cb54-7"><a href="regression-and-classification.html#cb54-7" tabindex="-1"></a></span>
<span id="cb54-8"><a href="regression-and-classification.html#cb54-8" tabindex="-1"></a>Coefficients<span class="sc">:</span></span>
<span id="cb54-9"><a href="regression-and-classification.html#cb54-9" tabindex="-1"></a>             Estimate Std. Error t value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>t<span class="sc">|</span>)    </span>
<span id="cb54-10"><a href="regression-and-classification.html#cb54-10" tabindex="-1"></a>(Intercept) <span class="sc">-</span><span class="fl">23242.75</span>    <span class="fl">3539.42</span>  <span class="sc">-</span><span class="fl">6.567</span> <span class="fl">6.43e-11</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb54-11"><a href="regression-and-classification.html#cb54-11" tabindex="-1"></a>age             <span class="fl">79.12</span>      <span class="fl">35.04</span>   <span class="fl">2.258</span>  <span class="fl">0.02405</span> <span class="sc">*</span>  </span>
<span id="cb54-12"><a href="regression-and-classification.html#cb54-12" tabindex="-1"></a>educ          <span class="fl">4025.78</span>     <span class="fl">212.41</span>  <span class="fl">18.953</span>  <span class="sc">&lt;</span> <span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb54-13"><a href="regression-and-classification.html#cb54-13" tabindex="-1"></a>sex2         <span class="sc">-</span><span class="fl">4473.33</span>    <span class="fl">1249.14</span>  <span class="sc">-</span><span class="fl">3.581</span>  <span class="fl">0.00035</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb54-14"><a href="regression-and-classification.html#cb54-14" tabindex="-1"></a><span class="sc">---</span></span>
<span id="cb54-15"><a href="regression-and-classification.html#cb54-15" tabindex="-1"></a>Signif. codes<span class="sc">:</span>  <span class="dv">0</span> ‘<span class="sc">**</span><span class="er">*</span>’ <span class="fl">0.001</span> ‘<span class="sc">**</span>’ <span class="fl">0.01</span> ‘<span class="sc">*</span>’ <span class="fl">0.05</span> ‘.’ <span class="fl">0.1</span> ‘ ’ <span class="dv">1</span></span>
<span id="cb54-16"><a href="regression-and-classification.html#cb54-16" tabindex="-1"></a></span>
<span id="cb54-17"><a href="regression-and-classification.html#cb54-17" tabindex="-1"></a>Residual standard error<span class="sc">:</span> <span class="dv">28790</span> on <span class="dv">2142</span> degrees of freedom</span>
<span id="cb54-18"><a href="regression-and-classification.html#cb54-18" tabindex="-1"></a>  (<span class="dv">202</span> observations deleted due to missingness)</span>
<span id="cb54-19"><a href="regression-and-classification.html#cb54-19" tabindex="-1"></a>Multiple R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.1487</span>,    Adjusted R<span class="sc">-</span>squared<span class="sc">:</span>  <span class="fl">0.1475</span> </span>
<span id="cb54-20"><a href="regression-and-classification.html#cb54-20" tabindex="-1"></a>F<span class="sc">-</span>statistic<span class="sc">:</span> <span class="fl">124.7</span> on <span class="dv">3</span> and <span class="dv">2142</span> DF,  p<span class="sc">-</span>value<span class="sc">:</span> <span class="er">&lt;</span> <span class="fl">2.2e-16</span></span></code></pre></div>
<p>Specifically, controlling for age and education, females (vs. males) on average are expected to earn -4473.33 dollars less in income. And this result is statistically significant given the small p-value.</p>
<p>However, more variables are not always good! Multicollinearity is an important diagnostic consideration. It occurs when two or more predictor variables in your model are highly correlated with each other, meaning they share redundant information. This redundancy can affect the stability and interpretability of your estimated coefficients.</p>
<p>You can check for multicollinearity in your R linear model using VIF:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="regression-and-classification.html#cb55-1" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;car&quot;</span>)  </span>
<span id="cb55-2"><a href="regression-and-classification.html#cb55-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb55-3"><a href="regression-and-classification.html#cb55-3" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(realinc <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> sex, <span class="at">data =</span> gss)</span>
<span id="cb55-4"><a href="regression-and-classification.html#cb55-4" tabindex="-1"></a>vif_values <span class="ot">&lt;-</span> <span class="fu">vif</span>(m4)</span>
<span id="cb55-5"><a href="regression-and-classification.html#cb55-5" tabindex="-1"></a><span class="fu">print</span>(vif_values)</span></code></pre></div>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="regression-and-classification.html#cb56-1" tabindex="-1"></a>   age     educ      sex </span>
<span id="cb56-2"><a href="regression-and-classification.html#cb56-2" tabindex="-1"></a><span class="fl">1.001432</span> <span class="fl">1.000524</span> <span class="fl">1.001562</span> </span></code></pre></div>
<p>The output will show a VIF for each predictor. If any values are high, you may need to consider removing or combining predictors. VIF &gt; 10 is generall indicative of serious multicollinearity that should be addressed.</p>
</div>
<div id="linear-regression-prediction" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Linear Regression Prediction<a href="regression-and-classification.html#linear-regression-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, imagine that you want to predict your own income when you graduate in the coming years using this model. Let’s collect data on what your age and education will be at the time of graduation and use the line to predict your future income. First, we create a data.frame with the necessary data.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="regression-and-classification.html#cb57-1" tabindex="-1"></a>you <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Name =</span> <span class="st">&quot;YOURNAMEHERE&quot;</span>, <span class="at">age =</span> <span class="dv">22</span>, <span class="at">educ =</span> <span class="dv">16</span>)</span></code></pre></div>
<p>As long as a data.frame has the same variables as those in a model, we can use the predict function to predict the expected outcomes of the respondents in the data.frame using the model. What is your predicted income?</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="regression-and-classification.html#cb58-1" tabindex="-1"></a><span class="fu">predict</span>(m3, you)</span></code></pre></div>
<p>To get a better prediction, more tailored to you and your background, we would need a more complex model with more variables!</p>
</div>
<div id="classification-logistic-regression" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Classification (Logistic Regression)<a href="regression-and-classification.html#classification-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression is used to predict the class (or category) of individuals based on one or multiple predictor variables (x). It is used to model a binary outcome, that is a variable, which can have only two possible values: 0 or 1, yes or no, diseased or non-diseased.</p>
<p><img src="week3_final_plot.png" /></p>
<p>The R function glm(), for generalized linear model, can be used to compute logistic regression. You need to specify the option family = binomial, which tells to R that we want to fit logistic regression.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="regression-and-classification.html#cb59-1" tabindex="-1"></a><span class="fu">mean</span>(gss<span class="sc">$</span>realinc,<span class="at">na.rm=</span>T)</span>
<span id="cb59-2"><a href="regression-and-classification.html#cb59-2" tabindex="-1"></a>gss<span class="sc">$</span>high_income <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb59-3"><a href="regression-and-classification.html#cb59-3" tabindex="-1"></a>gss<span class="sc">$</span>high_income[gss<span class="sc">$</span>realinc <span class="sc">&gt;</span> <span class="fl">33749.7</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb59-4"><a href="regression-and-classification.html#cb59-4" tabindex="-1"></a><span class="fu">table</span>(gss<span class="sc">$</span>high_income)</span>
<span id="cb59-5"><a href="regression-and-classification.html#cb59-5" tabindex="-1"></a></span>
<span id="cb59-6"><a href="regression-and-classification.html#cb59-6" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(high_income <span class="sc">~</span> educ, <span class="at">data =</span> gss, <span class="at">family =</span> binomial)</span>
<span id="cb59-7"><a href="regression-and-classification.html#cb59-7" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="regression-and-classification.html#cb60-1" tabindex="-1"></a>Call<span class="sc">:</span></span>
<span id="cb60-2"><a href="regression-and-classification.html#cb60-2" tabindex="-1"></a><span class="fu">glm</span>(<span class="at">formula =</span> high_income <span class="sc">~</span> educ, <span class="at">family =</span> binomial, <span class="at">data =</span> gss)</span>
<span id="cb60-3"><a href="regression-and-classification.html#cb60-3" tabindex="-1"></a></span>
<span id="cb60-4"><a href="regression-and-classification.html#cb60-4" tabindex="-1"></a>Coefficients<span class="sc">:</span></span>
<span id="cb60-5"><a href="regression-and-classification.html#cb60-5" tabindex="-1"></a>            Estimate Std. Error z value <span class="fu">Pr</span>(<span class="sc">&gt;</span><span class="er">|</span>z<span class="sc">|</span>)    </span>
<span id="cb60-6"><a href="regression-and-classification.html#cb60-6" tabindex="-1"></a>(Intercept) <span class="sc">-</span><span class="fl">4.67368</span>    <span class="fl">0.27149</span>  <span class="sc">-</span><span class="fl">17.21</span>   <span class="sc">&lt;</span><span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb60-7"><a href="regression-and-classification.html#cb60-7" tabindex="-1"></a>educ         <span class="fl">0.27891</span>    <span class="fl">0.01854</span>   <span class="fl">15.04</span>   <span class="sc">&lt;</span><span class="fl">2e-16</span> <span class="sc">**</span><span class="er">*</span></span>
<span id="cb60-8"><a href="regression-and-classification.html#cb60-8" tabindex="-1"></a><span class="sc">---</span></span>
<span id="cb60-9"><a href="regression-and-classification.html#cb60-9" tabindex="-1"></a>Signif. codes<span class="sc">:</span>  <span class="dv">0</span> ‘<span class="sc">**</span><span class="er">*</span>’ <span class="fl">0.001</span> ‘<span class="sc">**</span>’ <span class="fl">0.01</span> ‘<span class="sc">*</span>’ <span class="fl">0.05</span> ‘.’ <span class="fl">0.1</span> ‘ ’ <span class="dv">1</span></span>
<span id="cb60-10"><a href="regression-and-classification.html#cb60-10" tabindex="-1"></a></span>
<span id="cb60-11"><a href="regression-and-classification.html#cb60-11" tabindex="-1"></a>(Dispersion parameter <span class="cf">for</span> binomial family taken to be <span class="dv">1</span>)</span>
<span id="cb60-12"><a href="regression-and-classification.html#cb60-12" tabindex="-1"></a></span>
<span id="cb60-13"><a href="regression-and-classification.html#cb60-13" tabindex="-1"></a>    Null deviance<span class="sc">:</span> <span class="fl">2954.3</span>  on <span class="dv">2344</span>  degrees of freedom</span>
<span id="cb60-14"><a href="regression-and-classification.html#cb60-14" tabindex="-1"></a>Residual deviance<span class="sc">:</span> <span class="fl">2682.4</span>  on <span class="dv">2343</span>  degrees of freedom</span>
<span id="cb60-15"><a href="regression-and-classification.html#cb60-15" tabindex="-1"></a>  (<span class="dv">3</span> observations deleted due to missingness)</span>
<span id="cb60-16"><a href="regression-and-classification.html#cb60-16" tabindex="-1"></a>AIC<span class="sc">:</span> <span class="fl">2686.4</span></span>
<span id="cb60-17"><a href="regression-and-classification.html#cb60-17" tabindex="-1"></a></span>
<span id="cb60-18"><a href="regression-and-classification.html#cb60-18" tabindex="-1"></a>Number of Fisher Scoring iterations<span class="sc">:</span> <span class="dv">4</span></span></code></pre></div>
<p>The model is specified as:
<span class="math display">\[
logit(P(high income))= −4.67368 + 0.27891 × educ
\]</span></p>
<p>To convert this log-odds effect into an odds ratio, you exponentiate the coefficient:</p>
<p><span class="math display">\[
exp(0.27891)≈1.322
\]</span></p>
<p>This means that each additional unit (e.g., year) of education is associated with approximately a 32.2% (1.322-1) increase in the odds of high income. The very low p-value (&lt; 2e-16) suggests that the effect of education on the probability of high income is statistically significant.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="regression-and-classification.html#cb61-1" tabindex="-1"></a>you <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Name =</span> <span class="st">&quot;YOURNAMEHERE&quot;</span>, <span class="at">age =</span> <span class="dv">22</span>, <span class="at">educ =</span> <span class="dv">16</span>)</span></code></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="regression-and-classification.html#cb62-1" tabindex="-1"></a>predicted_probability <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> you, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb62-2"><a href="regression-and-classification.html#cb62-2" tabindex="-1"></a>predicted_probability</span></code></pre></div>
<p>The output will be the probability that you with the specified education level is in the high_income category.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="regression-and-classification.html#cb63-1" tabindex="-1"></a>predicted_probability</span>
<span id="cb63-2"><a href="regression-and-classification.html#cb63-2" tabindex="-1"></a><span class="fl">0.4474074</span> </span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="surveys-and-survey-experiments-with-qualtrics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="social-network-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yuzesui97/soc10_2025spring/edit/main/04-Regression_and_Classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/yuzesui97/soc10_2025spring/blob/main/04-Regression_and_Classification.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
