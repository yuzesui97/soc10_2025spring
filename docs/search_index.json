[["index.html", "Stanford Spring 2025 Intro to Computational Social Science About", " Stanford Spring 2025 Intro to Computational Social Science Yuze Sui 2025-03-31 About Figure Source: Katie Pacyna With an ever-growing amount of data collected on our online and offline behaviors—from our purchasing habits to our social interactions—we now have the unprecedented opportunity to study social life with remarkable precision. This hands-on course explores the promises and pitfalls of leveraging “big data” and algorithms to understand modern social and economic systems. Each week, we will examine key sociological topics such as inequality, discrimination, and polarization through the lens of computational methods, as well as delve into cutting-edge techniques in computational social science. The course culminates in a group research project that will equip you with skills ranging from designing and executing large-scale surveys and experiments to fitting machine learning models and analyzing diverse data types like social networks and natural language. No prior statistical or programming experience is required. This website is the place where you can find R progamming lab notes for the course. "],["intro-to-r.html", "Week 1 Intro to R 1.1 R Basics 1.2 Vectors 1.3 Loading Packages 1.4 Exploring and Visualizing Data", " Week 1 Intro to R This section is drafted based on Dr. Mark Hoffman’s previous SOC 10 lab notes. 1.1 R Basics This initial tutorial for R has two primary learning objectives. The first is to become affiliated with the R environment and the second is to learn how to extend the basic set of R functions to make it suitable for your own research purposes. The lessons we learn in this tutorial will serve as a strong basis for the those that follow, which focus on the actual analysis of data using R. Like most programming languages, R can serve as a calculator. We will use many of these basic mathematical operations when working with data. Let’s start with the basics: 2 + 2 return: [1] 4 3*(2+2) return: [1] 12 We use the assignment operator “&lt;-” to save the results in a vector for later. four &lt;- 2+2 sixteen &lt;- (2+2)^2 If we type the name of the vector, it will return its values. four return: [1] 4 Functions in R also have names. Functions save lines of codes! Later on, we will learn to write our own functions. For now, we can make use of the large body of default functions that exist within R. The most basic function is print. We can use it to output text in the console. print(&quot;Hello world!&quot;) return: [1] &quot;Hello world!&quot; log() is another useful function and it has two arguments, x and base. When you call the function log() you have to specify x (the input value), while base has a default of exp(1). log82 &lt;- log(x = 8, base = 2) If you don’t specify the arguments in their correct order, you must use argument=value form or else you will get a different result. log1 &lt;- log(8, base = 2) log2 &lt;- log(x = 8, base = 2) log3 &lt;- log(8, 2) log4 &lt;- log(base = 2, x = 8) log5 &lt;- log(2, 8) The cat function concatenates the R objects and prints them. cat(log1, log2, log3, log4, log5) return: 3 3 3 3 0.3333333 1.2 Vectors Vectors are the most basic object in R. They contain ordered elements of the same type. Vectors of size &gt; 1 are created using the “c” function. v &lt;- c(0,1,2,3,4,5,6,7,8,9) print(v) return: [1] 0 1 2 3 4 5 6 7 8 9 Computations on vectors are performed element-wise. v &lt;- v * 3 print(v) return: [1] 0 3 6 9 12 15 18 21 24 27 When we are working with a vector, we might to see what the fourth or fifth element is. We can use indexing to identify them. Indexing looks like this: v[4] return: [1] 9 v[5] return: [1] 12 Finally, we may wish to remove elements from a vector. We can use the subset function to do this. 1.3 Loading Packages Before we move on to datasets, rather than just vectors, let’s install some necessary packages. Packages are collections of R functions, data, and compiled code. They are built by members of the R community to add functionality to base R. Generally, if you wish you could do something with R, someone has built a package to do it already! We will use a few packages, some of which are built into R. We will need to install the others. For now, we just need to install tidyverse, which is the most complete data analysis and visualization package for R. To do so, we use the install.packages() function. # install.packages(&quot;tidyverse&quot;) The library function tells R to add the contents of the package to the current R session so that we can use it in our analyses. library(tidyverse) We will use the library() function every time we start a new R session. If R cannot find a function that you are sure is in a package you use, it normally means the package isn’t loaded or that you somehow misspelled the function name. 1.4 Exploring and Visualizing Data Now, let’s actually get our hands dirty and start analyzing data. The first thing we will need to do is to pick a data set to analyze. A good candidate is the General Social Survey (GSS) - a large-scale survey that sociologists have been administering since the 1960s, meant to gauge the changing social attitudes and practices of Americans over time. It is probably sociology’s most celebrated dataset, the subject of tens of thousands of papers since its inception. It isn’t quite big data (only a couple thousand respondets each year), but it is a good starting point for our first data exercises. To download the GSS, first navigate to: https://gss.norc.org/get-the-data/stata Then, click where it says 2018, under the heading Download Individual Year Data Sets (cross-section only). For now, drag the downloaded file to your Desktop. It is a STATA file, so we will have to use a special R package, foreign, to load it into R as well as the function, read.dta, from that package. A function is basically a command which helps you do something. Later on, we will work on writing our own functions, for now we will us the functions that other people’s packages supply for us. # load in the foreign package.. it comes with R library(foreign) # now we can load in the GSS data gss &lt;- read.dta(&quot;Data/GSS2018.dta&quot;) You can use the View() or head() functions to actually view the data and see what it looks like. ## abany abdefect abfelegl abhelp1 abhelp2 abhelp3 abhelp4 abhlth ## 1 no yes &lt;NA&gt; yes yes yes yes yes ## 2 yes yes it depends no no no no yes ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; yes no yes yes &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; should yes yes yes yes &lt;NA&gt; ## 5 no yes &lt;NA&gt; no no no yes yes ## 6 yes yes should yes yes yes yes yes ## abinspay abmedgov1 ## 1 people should be able the government should decide ## 2 people should not be able &lt;NA&gt; ## 3 people should not be able a woman and her medical professional should decide ## 4 people should be able &lt;NA&gt; ## 5 people should not be able &lt;NA&gt; ## 6 people should be able &lt;NA&gt; If you have ever worked with Excel before, this should look pretty familiar! In R, this is called a data.frame(). It is the most common way we will use to organize data. class(gss) return: [1] &quot;data.frame&quot; A data.frame is organized into rows and columns. Each row holds the data for a single respondent, whereas each column holds the data for a single variable (or question). Let’s see how many people (rows) and variables (columns) the data have. # Check the number of rows nrow(gss) return: [1] 2348 # Check the number of columns ncol(gss) return: [1] 1064 # Or both at the same time! dim(gss) return: [1] 2348 1064 We can find this information over in the environment too. The rows are numbered, while the columns have names. But if you look at the column names, they won’t make much sense. # check the column names colnames(gss) ## [1] &quot;abany&quot; &quot;abdefect&quot; &quot;abfelegl&quot; &quot;abhelp1&quot; ## [5] &quot;abhelp2&quot; &quot;abhelp3&quot; &quot;abhelp4&quot; &quot;abhlth&quot; ## [9] &quot;abinspay&quot; &quot;abmedgov1&quot; &quot;abmedgov2&quot; &quot;abmelegl&quot; ## [13] &quot;abmoral&quot; &quot;abnomore&quot; &quot;abpoor&quot; &quot;abpoorw&quot; ## [17] &quot;abrape&quot; &quot;absingle&quot; &quot;abstate1&quot; &quot;abstate2&quot; ## [21] &quot;acqntsex&quot; &quot;actssoc&quot; &quot;adults&quot; &quot;advfront&quot; ## [25] &quot;affrmact&quot; &quot;afraidof&quot; &quot;afterlif&quot; &quot;age&quot; ## [29] &quot;aged&quot; &quot;agekdbrn&quot; &quot;ancestrs&quot; &quot;arthrtis&quot; ## [33] &quot;astrolgy&quot; &quot;astrosci&quot; &quot;atheists&quot; &quot;attend&quot; ## [37] &quot;attend12&quot; &quot;attendma&quot; &quot;attendpa&quot; &quot;away1&quot; ## [41] &quot;away11&quot; &quot;away2&quot; &quot;away3&quot; &quot;away4&quot; ## [45] &quot;away5&quot; &quot;away6&quot; &quot;away7&quot; &quot;babies&quot; ## [49] &quot;backpain&quot; &quot;ballot&quot; &quot;balneg&quot; &quot;balpos&quot; ## [53] &quot;befair&quot; &quot;betrlang&quot; &quot;bible&quot; &quot;bigbang&quot; ## [57] &quot;bigbang1&quot; &quot;bigbang2&quot; &quot;bird&quot; &quot;birdb4&quot; ## [61] &quot;born&quot; &quot;boyorgrl&quot; &quot;breakdwn&quot; &quot;buddhsts&quot; ## [65] &quot;buyesop&quot; &quot;buyvalue&quot; &quot;cantrust&quot; &quot;cappun&quot; ## [69] &quot;cat&quot; &quot;catb4&quot; &quot;charactr&quot; &quot;chemgen&quot; ## [73] &quot;childs&quot; &quot;chldidel&quot; &quot;christns&quot; &quot;churhpow&quot; ## [77] &quot;class&quot; &quot;clergvte&quot; &quot;closeto1&quot; &quot;closeto2&quot; ## [81] &quot;closeto3&quot; &quot;closeto4&quot; &quot;closeto5&quot; &quot;cntctfam&quot; ## [85] &quot;cntctfrd&quot; &quot;cntctkid&quot; &quot;cntctpar&quot; &quot;cntctsib&quot; ## [89] &quot;codeg&quot; &quot;coden&quot; &quot;coeduc&quot; &quot;coevwork&quot; ## [93] &quot;cofund&quot; &quot;cohort&quot; &quot;cohrs1&quot; &quot;cohrs2&quot; ## [97] &quot;coind10&quot; &quot;coisco08&quot; &quot;cojew&quot; &quot;colath&quot; ## [101] &quot;colcom&quot; &quot;coldeg1&quot; &quot;colhomo&quot; &quot;colmil&quot; ## [105] &quot;colmslm&quot; &quot;colrac&quot; &quot;colsci&quot; &quot;colscinm&quot; ## [109] &quot;comfort&quot; &quot;company&quot; &quot;compperf&quot; &quot;comprend&quot; ## [113] &quot;compuse&quot; &quot;compwage&quot; &quot;conarmy&quot; &quot;conbiz&quot; ## [117] &quot;conbus&quot; &quot;conchurh&quot; &quot;conclerg&quot; &quot;concong&quot; ## [121] &quot;concourt&quot; &quot;condemnd&quot; &quot;condom&quot; &quot;condrift&quot; ## [125] &quot;coneduc&quot; &quot;conf2f&quot; &quot;confed&quot; &quot;confinan&quot; ## [129] &quot;coninc&quot; &quot;conjudge&quot; &quot;conlabor&quot; &quot;conlegis&quot; ## [133] &quot;conmedic&quot; &quot;conpress&quot; &quot;conrinc&quot; &quot;conschls&quot; ## [137] &quot;consci&quot; &quot;consent&quot; &quot;contv&quot; &quot;conwkday&quot; ## [141] &quot;coocc10&quot; &quot;coop&quot; &quot;coother&quot; &quot;copres10&quot; ## [145] &quot;copres105plus&quot; &quot;corel&quot; &quot;cosei10&quot; &quot;cosei10educ&quot; ## [149] &quot;cosei10inc&quot; &quot;courts&quot; &quot;cowrkhlp&quot; &quot;cowrkint&quot; ## [153] &quot;cowrkslf&quot; &quot;cowrksta&quot; &quot;crack30&quot; &quot;dangoth1&quot; ## [157] &quot;dangoth2&quot; &quot;dangoth3&quot; &quot;dangoth4&quot; &quot;dangoth5&quot; ## [161] &quot;dangroth&quot; &quot;dangrslf&quot; &quot;dangslf1&quot; &quot;dangslf2&quot; ## [165] &quot;dangslf3&quot; &quot;dangslf4&quot; &quot;dangslf5&quot; &quot;dateintv&quot; ## [169] &quot;decmoney&quot; &quot;dectreat&quot; &quot;defpensn&quot; &quot;degree&quot; ## [173] &quot;demands&quot; &quot;denkid&quot; &quot;denom&quot; &quot;denom16&quot; ## [177] &quot;depress&quot; &quot;deptperf&quot; &quot;diabetes&quot; &quot;diagnosd&quot; ## [181] &quot;difrel&quot; &quot;dinefrds&quot; &quot;dipged&quot; &quot;discaff&quot; ## [185] &quot;discaffm&quot; &quot;discaffw&quot; &quot;disrspct&quot; &quot;divlaw&quot; ## [189] &quot;divorce&quot; &quot;dofirst&quot; &quot;dog&quot; &quot;dogb4&quot; ## [193] &quot;dwelling&quot; &quot;dwellpre&quot; &quot;dwelown&quot; &quot;dwelown16&quot; ## [197] &quot;earnrs&quot; &quot;earthsun&quot; &quot;educ&quot; &quot;egomeans&quot; ## [201] &quot;electron&quot; &quot;emailhr&quot; &quot;emailmin&quot; &quot;emoprobs&quot; ## [205] &quot;empinput&quot; &quot;emptrain&quot; &quot;endsmeet&quot; &quot;eqwlth&quot; ## [209] &quot;esop&quot; &quot;esopnot&quot; &quot;eth1&quot; &quot;eth2&quot; ## [213] &quot;eth3&quot; &quot;ethnic&quot; &quot;ethnum&quot; &quot;evcrack&quot; ## [217] &quot;evidu&quot; &quot;evolved&quot; &quot;evolved2&quot; &quot;evpaidsx&quot; ## [221] &quot;evstray&quot; &quot;evwork&quot; &quot;expdesgn&quot; &quot;exptext&quot; ## [225] &quot;extr2017&quot; &quot;extrapay&quot; &quot;extraval&quot; &quot;extrayr&quot; ## [229] &quot;fair&quot; &quot;fairearn&quot; &quot;famdif16&quot; &quot;famgen&quot; ## [233] &quot;family16&quot; &quot;fammhneg&quot; &quot;fampress&quot; &quot;famvswk&quot; ## [237] &quot;famwkoff&quot; &quot;fatalism&quot; &quot;fatigue&quot; &quot;fear&quot; ## [241] &quot;fechld&quot; &quot;feelevel&quot; &quot;feelrel&quot; &quot;feeused&quot; ## [245] &quot;fefam&quot; &quot;fehire&quot; &quot;fejobaff&quot; &quot;fepol&quot; ## [249] &quot;fepresch&quot; &quot;finalter&quot; &quot;finrela&quot; &quot;firstyou&quot; ## [253] &quot;fish&quot; &quot;fishb4&quot; &quot;form&quot; &quot;formwt&quot; ## [257] &quot;fringeok&quot; &quot;frndsex&quot; &quot;fucitzn&quot; &quot;fund&quot; ## [261] &quot;fund16&quot; &quot;gender1&quot; &quot;gender10&quot; &quot;gender11&quot; ## [265] &quot;gender12&quot; &quot;gender2&quot; &quot;gender3&quot; &quot;gender4&quot; ## [269] &quot;gender5&quot; &quot;gender6&quot; &quot;gender7&quot; &quot;gender8&quot; ## [273] &quot;gender9&quot; &quot;geneabrt2&quot; &quot;genegen&quot; &quot;genegoo2&quot; ## [277] &quot;geneself2&quot; &quot;genetics&quot; &quot;genetst1&quot; &quot;getahead&quot; ## [281] &quot;goat&quot; &quot;goatb4&quot; &quot;god&quot; &quot;godchnge&quot; ## [285] &quot;godmeans&quot; &quot;godswill&quot; &quot;goodlife&quot; &quot;goveqinc&quot; ## [289] &quot;govlazy&quot; &quot;govvsrel&quot; &quot;granborn&quot; &quot;grass&quot; ## [293] &quot;gunlaw&quot; &quot;handmove&quot; &quot;hapcohab&quot; &quot;hapmar&quot; ## [297] &quot;happy&quot; &quot;hapunhap&quot; &quot;haveinfo&quot; &quot;health&quot; ## [301] &quot;health1&quot; &quot;healthissp&quot; &quot;heaven&quot; &quot;hefinfo&quot; ## [305] &quot;height&quot; &quot;hell&quot; &quot;helpblk&quot; &quot;helpfrds&quot; ## [309] &quot;helpful&quot; &quot;helpnot&quot; &quot;helpoth&quot; &quot;helppoor&quot; ## [313] &quot;helpsick&quot; &quot;hhrace&quot; &quot;hhtype&quot; &quot;hhtype1&quot; ## [317] &quot;hindus&quot; &quot;hispanic&quot; &quot;hivtest&quot; &quot;hivtest1&quot; ## [321] &quot;hivtest2&quot; &quot;hlpadvce&quot; &quot;hlpdown&quot; &quot;hlpequip&quot; ## [325] &quot;hlphome&quot; &quot;hlpjob&quot; &quot;hlploan&quot; &quot;hlppaper&quot; ## [329] &quot;hlpresde&quot; &quot;hlpsick&quot; &quot;hlpsickr&quot; &quot;hlpsococ&quot; ## [333] &quot;hlthdays&quot; &quot;hlthmntl&quot; &quot;hlthphys&quot; &quot;hlthstrt&quot; ## [337] &quot;homosex&quot; &quot;homosex1&quot; &quot;hompop&quot; &quot;horse&quot; ## [341] &quot;horseb4&quot; &quot;hotcore&quot; &quot;hrs1&quot; &quot;hrs2&quot; ## [345] &quot;hrsrelax&quot; &quot;hsbio&quot; &quot;hschem&quot; &quot;hsmath&quot; ## [349] &quot;hsphys&quot; &quot;huadd&quot; &quot;huaddwhy&quot; &quot;hubbywrk&quot; ## [353] &quot;huclean&quot; &quot;hunt&quot; &quot;hunt1&quot; &quot;hurtatwk&quot; ## [357] &quot;hurtoth&quot; &quot;hurtself&quot; &quot;hvylift&quot; &quot;hyperten&quot; ## [361] &quot;id&quot; &quot;idu30&quot; &quot;if12who&quot; &quot;if16who&quot; ## [365] &quot;imbalnce&quot; &quot;imprvown&quot; &quot;imprvtrt&quot; &quot;incgap&quot; ## [369] &quot;incom16&quot; &quot;income&quot; &quot;income16&quot; &quot;incuspop&quot; ## [373] &quot;indperf&quot; &quot;indus10&quot; &quot;indusgen&quot; &quot;intage&quot; ## [377] &quot;intcntct&quot; &quot;intecon&quot; &quot;inteduc&quot; &quot;intenvir&quot; ## [381] &quot;intethn&quot; &quot;intfarm&quot; &quot;inthisp&quot; &quot;intid&quot; ## [385] &quot;intintl&quot; &quot;intlblks&quot; &quot;intlhsps&quot; &quot;intlwhts&quot; ## [389] &quot;intmed&quot; &quot;intmil&quot; &quot;intrace1&quot; &quot;intrace2&quot; ## [393] &quot;intrace3&quot; &quot;intsci&quot; &quot;intsex&quot; &quot;intspace&quot; ## [397] &quot;inttech&quot; &quot;intyrs&quot; &quot;isco08&quot; &quot;isco88&quot; ## [401] &quot;issp&quot; &quot;jew&quot; &quot;jew16&quot; &quot;jews&quot; ## [405] &quot;jobfind&quot; &quot;jobfind1&quot; &quot;joblose&quot; &quot;jobsecok&quot; ## [409] &quot;kidpars&quot; &quot;kidsinhh&quot; &quot;kidssol&quot; &quot;knowschd&quot; ## [413] &quot;knowwhat&quot; &quot;knwbus&quot; &quot;knwclenr&quot; &quot;knwcop&quot; ## [417] &quot;knwcuttr&quot; &quot;knwexec&quot; &quot;knwhrman&quot; &quot;knwlawyr&quot; ## [421] &quot;knwmchnc&quot; &quot;knwmw1&quot; &quot;knwmw2&quot; &quot;knwmw3&quot; ## [425] &quot;knwmw4&quot; &quot;knwmw5&quot; &quot;knwnurse&quot; &quot;knwtcher&quot; ## [429] &quot;laidoff&quot; &quot;lasers&quot; &quot;learnnew&quot; &quot;letdie1&quot; ## [433] &quot;letin1a&quot; &quot;libath&quot; &quot;libcom&quot; &quot;libhomo&quot; ## [437] &quot;libmil&quot; &quot;libmslm&quot; &quot;librac&quot; &quot;life&quot; ## [441] &quot;lifein5&quot; &quot;lifenow&quot; &quot;liveblks&quot; &quot;lngthinv&quot; ## [445] &quot;localnum&quot; &quot;lonely1&quot; &quot;lonely2&quot; &quot;lonely3&quot; ## [449] &quot;madeg&quot; &quot;madenkid&quot; &quot;maeduc&quot; &quot;maind10&quot; ## [453] &quot;maisco08&quot; &quot;maisco88&quot; &quot;major1&quot; &quot;major2&quot; ## [457] &quot;majorcol&quot; &quot;makefrnd&quot; &quot;maleornt&quot; &quot;manvsemp&quot; ## [461] &quot;maocc10&quot; &quot;mapres10&quot; &quot;mapres105plus&quot; &quot;mar1&quot; ## [465] &quot;mar11&quot; &quot;mar12&quot; &quot;mar2&quot; &quot;mar3&quot; ## [469] &quot;mar4&quot; &quot;mar5&quot; &quot;mar6&quot; &quot;mar7&quot; ## [473] &quot;mar8&quot; &quot;mar9&quot; &quot;marasian&quot; &quot;marblk&quot; ## [477] &quot;marcohab&quot; &quot;marelkid&quot; &quot;marhisp&quot; &quot;marhomo&quot; ## [481] &quot;marital&quot; &quot;martype&quot; &quot;marwht&quot; &quot;masei10&quot; ## [485] &quot;masei10educ&quot; &quot;masei10inc&quot; &quot;matesex&quot; &quot;mawrkgrw&quot; ## [489] &quot;mawrkslf&quot; &quot;mcsds1&quot; &quot;mcsds2&quot; &quot;mcsds3&quot; ## [493] &quot;mcsds4&quot; &quot;mcsds5&quot; &quot;mcsds6&quot; &quot;mcsds7&quot; ## [497] &quot;meddoc&quot; &quot;mentldoc&quot; &quot;mentlhos&quot; &quot;mentlill&quot; ## [501] &quot;mentloth&quot; &quot;meovrwrk&quot; &quot;mhdiagno&quot; &quot;mhp1r1&quot; ## [505] &quot;mhp1r2&quot; &quot;mhp2r1&quot; &quot;mhp2r2&quot; &quot;mhp3r1&quot; ## [509] &quot;mhp3r2&quot; &quot;mhp4r1&quot; &quot;mhp4r2&quot; &quot;mhp5r1&quot; ## [513] &quot;mhp5r2&quot; &quot;mhtreat1&quot; &quot;mhtreat2&quot; &quot;mhtreat3&quot; ## [517] &quot;mhtreat4&quot; &quot;mhtreat5&quot; &quot;mhtreatd&quot; &quot;mhunsure&quot; ## [521] &quot;miracles&quot; &quot;misswork&quot; &quot;mnthsusa&quot; &quot;mntlhlth&quot; ## [525] &quot;mobile16&quot; &quot;mode&quot; &quot;moredays&quot; &quot;muslims&quot; ## [529] &quot;mustdoc&quot; &quot;musthosp&quot; &quot;mustmed&quot; &quot;mustwork&quot; ## [533] &quot;mygoals&quot; &quot;myprobs1&quot; &quot;myprobs2&quot; &quot;myprobs3&quot; ## [537] &quot;myprobs4&quot; &quot;myprobs5&quot; &quot;myskills&quot; &quot;mywaygod&quot; ## [541] &quot;nanoben&quot; &quot;nanoharm&quot; &quot;nanowill&quot; &quot;nataccess&quot; ## [545] &quot;natactive&quot; &quot;nataid&quot; &quot;nataidy&quot; &quot;natarms&quot; ## [549] &quot;natarmsy&quot; &quot;natchld&quot; &quot;natcity&quot; &quot;natcityy&quot; ## [553] &quot;natcrime&quot; &quot;natcrimy&quot; &quot;natdrug&quot; &quot;natdrugy&quot; ## [557] &quot;nateduc&quot; &quot;nateducy&quot; &quot;natenrgy&quot; &quot;natenvir&quot; ## [561] &quot;natenviy&quot; &quot;natfare&quot; &quot;natfarey&quot; &quot;natheal&quot; ## [565] &quot;nathealy&quot; &quot;natlack&quot; &quot;natmass&quot; &quot;natmeet&quot; ## [569] &quot;natnotice&quot; &quot;natpark&quot; &quot;natrace&quot; &quot;natracey&quot; ## [573] &quot;natrelax&quot; &quot;natroad&quot; &quot;natsat&quot; &quot;natsci&quot; ## [577] &quot;natsoc&quot; &quot;natspac&quot; &quot;natspacy&quot; &quot;nattime&quot; ## [581] &quot;nattimeok&quot; &quot;natviews&quot; &quot;neisafe&quot; &quot;newfrds&quot; ## [585] &quot;news&quot; &quot;newsfrom&quot; &quot;nextgen&quot; &quot;nihilism&quot; ## [589] &quot;notsmart&quot; &quot;ntwkhard&quot; &quot;nukegen&quot; &quot;numcong&quot; ## [593] &quot;numemps&quot; &quot;numlangs&quot; &quot;nummen&quot; &quot;numorg&quot; ## [597] &quot;numpets&quot; &quot;numwomen&quot; &quot;obey&quot; &quot;occ10&quot; ## [601] &quot;odds1&quot; &quot;odds2&quot; &quot;old1&quot; &quot;old10&quot; ## [605] &quot;old11&quot; &quot;old12&quot; &quot;old2&quot; &quot;old3&quot; ## [609] &quot;old4&quot; &quot;old5&quot; &quot;old6&quot; &quot;old7&quot; ## [613] &quot;old8&quot; &quot;old9&quot; &quot;opdevel&quot; &quot;otcmed&quot; ## [617] &quot;oth16&quot; &quot;other&quot; &quot;othersex&quot; &quot;othlang&quot; ## [621] &quot;othlang1&quot; &quot;othlang2&quot; &quot;othmhneg&quot; &quot;othpet&quot; ## [625] &quot;othpetb4&quot; &quot;oversamp&quot; &quot;overwork&quot; &quot;owngun&quot; ## [629] &quot;ownstock&quot; &quot;padeg&quot; &quot;padenkid&quot; &quot;paeduc&quot; ## [633] &quot;paidsex&quot; &quot;painarms&quot; &quot;paind10&quot; &quot;paisco08&quot; ## [637] &quot;paisco88&quot; &quot;paocc10&quot; &quot;papres10&quot; &quot;papres105plus&quot; ## [641] &quot;parborn&quot; &quot;parelkid&quot; &quot;parsol&quot; &quot;partfull&quot; ## [645] &quot;partlsc&quot; &quot;partners&quot; &quot;partnrs5&quot; &quot;partpart&quot; ## [649] &quot;partteam&quot; &quot;partvol&quot; &quot;partyid&quot; &quot;pasei10&quot; ## [653] &quot;pasei10educ&quot; &quot;pasei10inc&quot; &quot;pawrkslf&quot; &quot;petb4&quot; ## [657] &quot;petb4cmfrt&quot; &quot;petb4fam&quot; &quot;petb4ply&quot; &quot;petcmfrt&quot; ## [661] &quot;petfam&quot; &quot;petplay&quot; &quot;phase&quot; &quot;phone&quot; ## [665] &quot;phyeffrt&quot; &quot;physacts&quot; &quot;physhlth&quot; &quot;physill&quot; ## [669] &quot;pig&quot; &quot;pigb4&quot; &quot;pikupsex&quot; &quot;pilingup&quot; ## [673] &quot;pillok&quot; &quot;pistol&quot; &quot;polabuse&quot; &quot;polattak&quot; ## [677] &quot;poleff11&quot; &quot;polescap&quot; &quot;polhitok&quot; &quot;polmurdr&quot; ## [681] &quot;polviews&quot; &quot;poorserv&quot; &quot;popespks&quot; &quot;popular&quot; ## [685] &quot;pornlaw&quot; &quot;posslq&quot; &quot;posslqy&quot; &quot;postlife&quot; ## [689] &quot;pray&quot; &quot;prayer&quot; &quot;prayfreq&quot; &quot;premarsx&quot; ## [693] &quot;pres12&quot; &quot;pres16&quot; &quot;prestg10&quot; &quot;prestg105plus&quot; ## [697] &quot;preteen&quot; &quot;prodctiv&quot; &quot;promtefr&quot; &quot;promteok&quot; ## [701] &quot;proudemp&quot; &quot;prvdhlth&quot; &quot;prvdold&quot; &quot;quallife&quot; ## [705] &quot;racdif1&quot; &quot;racdif2&quot; &quot;racdif3&quot; &quot;racdif4&quot; ## [709] &quot;race&quot; &quot;racecen1&quot; &quot;racecen2&quot; &quot;racecen3&quot; ## [713] &quot;raclive&quot; &quot;racopen&quot; &quot;racwork&quot; &quot;radioact&quot; ## [717] &quot;random&quot; &quot;rank&quot; &quot;ratepain&quot; &quot;ratetone&quot; ## [721] &quot;realinc&quot; &quot;realrinc&quot; &quot;reborn&quot; &quot;reg16&quot; ## [725] &quot;region&quot; &quot;relactiv&quot; &quot;relate1&quot; &quot;relate10&quot; ## [729] &quot;relate11&quot; &quot;relate12&quot; &quot;relate2&quot; &quot;relate3&quot; ## [733] &quot;relate4&quot; &quot;relate5&quot; &quot;relate6&quot; &quot;relate7&quot; ## [737] &quot;relate8&quot; &quot;relate9&quot; &quot;relatsex&quot; &quot;relext1&quot; ## [741] &quot;relext3&quot; &quot;relgenbar&quot; &quot;relgeneq&quot; &quot;relhh1&quot; ## [745] &quot;relhh10&quot; &quot;relhh11&quot; &quot;relhh12&quot; &quot;relhh2&quot; ## [749] &quot;relhh3&quot; &quot;relhh4&quot; &quot;relhh5&quot; &quot;relhh6&quot; ## [753] &quot;relhh7&quot; &quot;relhh8&quot; &quot;relhh9&quot; &quot;relhhd1&quot; ## [757] &quot;relhhd10&quot; &quot;relhhd11&quot; &quot;relhhd12&quot; &quot;relhhd2&quot; ## [761] &quot;relhhd3&quot; &quot;relhhd4&quot; &quot;relhhd5&quot; &quot;relhhd6&quot; ## [765] &quot;relhhd7&quot; &quot;relhhd8&quot; &quot;relhhd9&quot; &quot;relig&quot; ## [769] &quot;relig16&quot; &quot;religcon&quot; &quot;religint&quot; &quot;religkid&quot; ## [773] &quot;reliten&quot; &quot;relmarry&quot; &quot;relobjct&quot; &quot;relpast&quot; ## [777] &quot;relpersn&quot; &quot;relrlvnt&quot; &quot;relscrpt&quot; &quot;relsp1&quot; ## [781] &quot;relsp10&quot; &quot;relsp11&quot; &quot;relsp12&quot; &quot;relsp2&quot; ## [785] &quot;relsp3&quot; &quot;relsp4&quot; &quot;relsp5&quot; &quot;relsp6&quot; ## [789] &quot;relsp7&quot; &quot;relsp8&quot; &quot;relsp9&quot; &quot;relsprt&quot; ## [793] &quot;reptile&quot; &quot;reptileb4&quot; &quot;res16&quot; &quot;respect&quot; ## [797] &quot;respnum&quot; &quot;respond&quot; &quot;rfamlook&quot; &quot;rgroomed&quot; ## [801] &quot;rhlthend&quot; &quot;richwork&quot; &quot;rifle&quot; &quot;rincblls&quot; ## [805] &quot;rincom16&quot; &quot;rincome&quot; &quot;rlooks&quot; &quot;rowngun&quot; ## [809] &quot;rplace&quot; &quot;rvisitor&quot; &quot;rweight&quot; &quot;rxmed&quot; ## [813] &quot;safefrst&quot; &quot;safehlth&quot; &quot;safetywk&quot; &quot;sampcode&quot; ## [817] &quot;sample&quot; &quot;satfam7&quot; &quot;satfin&quot; &quot;satjob&quot; ## [821] &quot;satjob1&quot; &quot;satlife&quot; &quot;satsoc&quot; &quot;savesoul&quot; ## [825] &quot;scibnfts&quot; &quot;scientbe&quot; &quot;scientgo&quot; &quot;scienthe&quot; ## [829] &quot;scientod&quot; &quot;scifrom&quot; &quot;scinews1&quot; &quot;scinews2&quot; ## [833] &quot;scinews3&quot; &quot;scistudy&quot; &quot;scitext&quot; &quot;secondwk&quot; ## [837] &quot;seeksci&quot; &quot;seetalk1&quot; &quot;seetalk2&quot; &quot;seetalk3&quot; ## [841] &quot;seetalk4&quot; &quot;seetalk5&quot; &quot;sei10&quot; &quot;sei10educ&quot; ## [845] &quot;sei10inc&quot; &quot;selfhelp&quot; &quot;seriousp&quot; &quot;severe1&quot; ## [849] &quot;severe2&quot; &quot;severe3&quot; &quot;severe4&quot; &quot;severe5&quot; ## [853] &quot;sex&quot; &quot;sexbirth&quot; &quot;sexeduc&quot; &quot;sexfreq&quot; ## [857] &quot;sexnow&quot; &quot;sexornt&quot; &quot;sexsex&quot; &quot;sexsex5&quot; ## [861] &quot;shotgun&quot; &quot;sibs&quot; &quot;size&quot; &quot;slfmangd&quot; ## [865] &quot;slpprblm&quot; &quot;smallgap&quot; &quot;smammal&quot; &quot;smammalb4&quot; ## [869] &quot;socbar&quot; &quot;socfrend&quot; &quot;socommun&quot; &quot;socrel&quot; ## [873] &quot;solarrev&quot; &quot;spaneng&quot; &quot;spanint&quot; &quot;spanking&quot; ## [877] &quot;spanself&quot; &quot;spdeg&quot; &quot;spden&quot; &quot;speduc&quot; ## [881] &quot;spevwork&quot; &quot;spfalook&quot; &quot;spfund&quot; &quot;sphealer&quot; ## [885] &quot;sphrs1&quot; &quot;sphrs2&quot; &quot;spind10&quot; &quot;spisco08&quot; ## [889] &quot;spisco88&quot; &quot;spjew&quot; &quot;spkath&quot; &quot;spkcom&quot; ## [893] &quot;spkhomo&quot; &quot;spklang&quot; &quot;spkmil&quot; &quot;spkmslm&quot; ## [897] &quot;spkrac&quot; &quot;splive&quot; &quot;spocc10&quot; &quot;spother&quot; ## [901] &quot;sppres10&quot; &quot;sppres105plus&quot; &quot;sprel&quot; &quot;sprtprsn&quot; ## [905] &quot;spsei10&quot; &quot;spsei10educ&quot; &quot;spsei10inc&quot; &quot;spvtrfair&quot; ## [909] &quot;spwksup&quot; &quot;spwrkslf&quot; &quot;spwrksta&quot; &quot;srcbelt&quot; ## [913] &quot;stockops&quot; &quot;stockval&quot; &quot;stress&quot; &quot;stress12&quot; ## [917] &quot;stresses&quot; &quot;strredpg&quot; &quot;suicide1&quot; &quot;suicide2&quot; ## [921] &quot;suicide3&quot; &quot;suicide4&quot; &quot;supcares&quot; &quot;supervis&quot; ## [925] &quot;suphelp&quot; &quot;tax&quot; &quot;teamsafe&quot; &quot;teens&quot; ## [929] &quot;teensex&quot; &quot;tempgen&quot; &quot;theism&quot; &quot;thnkself&quot; ## [933] &quot;threaten&quot; &quot;tlkclrgy&quot; &quot;tlkfam&quot; &quot;toofast&quot; ## [937] &quot;toofewwk&quot; &quot;trbigbus&quot; &quot;trcourts&quot; &quot;trdunion&quot; ## [941] &quot;trust&quot; &quot;trustman&quot; &quot;trustsci&quot; &quot;trynewjb&quot; ## [945] &quot;tvhours&quot; &quot;unemp&quot; &quot;unhappy&quot; &quot;union&quot; ## [949] &quot;union1&quot; &quot;unrelat&quot; &quot;upsdowns&quot; &quot;upset&quot; ## [953] &quot;uscitzn&quot; &quot;usedup&quot; &quot;usetech&quot; &quot;usewww&quot; ## [957] &quot;uswary&quot; &quot;version&quot; &quot;vetyears&quot; &quot;vigfrnd&quot; ## [961] &quot;viggrp&quot; &quot;viglabel&quot; &quot;vigmar&quot; &quot;vignei&quot; ## [965] &quot;vigsoc&quot; &quot;vigversn&quot; &quot;vigwork&quot; &quot;viruses&quot; ## [969] &quot;visitors&quot; &quot;visnhist&quot; &quot;vissci&quot; &quot;vistholy&quot; ## [973] &quot;viszoo&quot; &quot;vote12&quot; &quot;vote16&quot; &quot;vpsu&quot; ## [977] &quot;vstrat&quot; &quot;watergen&quot; &quot;waypaid&quot; &quot;wayraise&quot; ## [981] &quot;wealth&quot; &quot;webmob&quot; &quot;weekswrk&quot; &quot;weight&quot; ## [985] &quot;where1&quot; &quot;where11&quot; &quot;where2&quot; &quot;where3&quot; ## [989] &quot;where4&quot; &quot;where5&quot; &quot;where6&quot; &quot;where7&quot; ## [993] &quot;whoelse1&quot; &quot;whoelse2&quot; &quot;whoelse3&quot; &quot;whoelse4&quot; ## [997] &quot;whoelse5&quot; &quot;whoelse6&quot; &quot;whynopet&quot; &quot;whywkhme&quot; ## [1001] &quot;widowed&quot; &quot;wkageism&quot; &quot;wkdecide&quot; &quot;wkfreedm&quot; ## [1005] &quot;wkharoth&quot; &quot;wkharsex&quot; &quot;wkpraise&quot; &quot;wkracism&quot; ## [1009] &quot;wksexism&quot; &quot;wksmooth&quot; &quot;wksub&quot; &quot;wksub1&quot; ## [1013] &quot;wksubs&quot; &quot;wksubs1&quot; &quot;wksup&quot; &quot;wksup1&quot; ## [1017] &quot;wksups&quot; &quot;wksups1&quot; &quot;wkvsfam&quot; &quot;wlthblks&quot; ## [1021] &quot;wlthhsps&quot; &quot;wlthwhts&quot; &quot;worda&quot; &quot;wordb&quot; ## [1025] &quot;wordc&quot; &quot;wordd&quot; &quot;worde&quot; &quot;wordf&quot; ## [1029] &quot;wordg&quot; &quot;wordh&quot; &quot;wordi&quot; &quot;wordj&quot; ## [1033] &quot;wordsum&quot; &quot;workblks&quot; &quot;workdiff&quot; &quot;workfast&quot; ## [1037] &quot;workfor1&quot; &quot;workhard&quot; &quot;workhsps&quot; &quot;workwhts&quot; ## [1041] &quot;wrkgovt&quot; &quot;wrkhome&quot; &quot;wrksched&quot; &quot;wrkslf&quot; ## [1045] &quot;wrkslffam&quot; &quot;wrkstat&quot; &quot;wrktime&quot; &quot;wrktype&quot; ## [1049] &quot;wrkwayup&quot; &quot;wtss&quot; &quot;wtssall&quot; &quot;wtssnr&quot; ## [1053] &quot;wwwhr&quot; &quot;wwwmin&quot; &quot;xmarsex&quot; &quot;xmarsex1&quot; ## [1057] &quot;xmovie&quot; &quot;xnorcsiz&quot; &quot;year&quot; &quot;yearsjob&quot; ## [1061] &quot;yearsusa&quot; &quot;yearval&quot; &quot;yousup&quot; &quot;zodiac&quot; They are like a secret code - if you know what the code means then you can figure out what information a variable holds. But how do we figure that out? We have to use a codebook! Head over to https://gssdataexplorer.norc.org/variables/vfilter for an accessible online codebook. You can search for keywords to see if it has variables that you care about. Here are some random suggestions: * age * income * partyid * sex * race * hrs1 We can evaluate these variables more closely using various R functions. For example, what are respondents’ average age? We use the mean function for that, and specify the na.rm = T option to tell R that respodent’s who didn’t report an age should be ignored. mean(gss$age, na.rm = T) return: [1] 48.97138 Interesting - almost 50 years old! By comparison, the average age in the US is roughly 38 years old. This might signal that the GSS is skewed towards older respondents, but we have to remember that babies do not take surveys. We can use the summary function to get a host of information about a variable’s distribution. For example, below we apply it to partyid - a measure of political affiliation which goes from 1 (Strong Demcorat) to 7 (Strong Republican) [the reason that it is different in practice than what the codebook says is because of how R converts factors to numerics, which we have to do to examine the scale quantitatively – specifically, the first option of a factor in R will be a 1 rather than a 0]. The median is a 4, which is dabsmack in the middle of the scale. The median respondent is an independent with no reported party leanings. summary(as.numeric(gss$partyid), na.rm = T) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 2.000 4.000 3.883 6.000 8.000 33 Cool! What if we wanted to see how two variables relate to one another? For example, we have heard from pundits that racial background strongly shapes political leaning in the today’s America – does the GSS confirm that conclusion? For comparison, we can also look at how people of varioius racial backgrounds differ by age. To achieve this, we first have to group the data by race using the group_by function from tidyverse. gss &lt;- gss %&gt;% group_by(race) Now that respondents are grouped by race, we can use the summarize function to evaluate each group’s average party identification and age. vars_by_race &lt;- gss %&gt;% summarise( partyid = mean(as.numeric(partyid), na.rm = T), age = mean(age, na.rm = T) ) vars_by_race ## # A tibble: 3 × 3 ## race partyid age ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 white 4.25 50.5 ## 2 black 2.45 46.0 ## 3 other 3.61 43.3 With respect to politics - just as we expected! People who identify as white also tend to be a bit older on average than people who identify as black. Instead of just looking at the averages, we could visualize them using a simple barplot. We will use the ggplot function for this, which is one of the most useful suite of functions for visualization in all of R. ggplot(vars_by_race, aes(x=race, y=partyid, fill=race)) + geom_bar(stat=&quot;identity&quot;) It is a complex function, so let’s break it down. The first argument is the data we want to visualize itself - vars_by_race. Then we need to establish the aesthetics (i.e. aes()). x is the x variable (i.e. the variable on the x-axis of the graph), y is the y variable (i.e. the varible on the y axis of the graph), and fill is the variable we want to use to color the bars (race). Now that the aesthetics are established, we tell ggplot what kind of plot we want (geom_bar specifies a bar plot). And stat = “identity” means we want to graph the values as they are in the data we provided, rather than trying to do something else with them. Let me now use another example (“marital” variable, which represents marital status) to demonstrate how we can further beautify a bar plot. Specifically, I will show how to edit the labels on the X-axis. If we directly plot it using the following code, you will notice the x-axis labels are not readable in the sense we do not know what different numbers represent. gss$marital &lt;- as.character(gss$marital) ggplot(gss, aes(x = marital)) + geom_bar(fill = &quot;blue&quot;,) + labs(title = &quot;Marital Status of Respondents&quot;, x = &quot;Marital Status&quot;, y = &quot;Frequency&quot;) Alternatively, we can add a scale_x_discrete() option to manually label the different values: gss$marital &lt;- as.character(gss$marital) ggplot(gss, aes(x = marital)) + geom_bar(fill = &quot;blue&quot;,) + scale_x_discrete(labels=c(&quot;1&quot; = &quot;Married&quot;, &quot;2&quot; = &quot;Widowed&quot;,&quot;3&quot; = &quot;Divorced&quot;, &quot;4&quot; = &quot;Separated&quot;, &quot;5&quot; = &quot;Never married&quot;)) + labs(title = &quot;Marital Status of Respondents&quot;, x = &quot;Marital Status&quot;, y = &quot;Frequency&quot;) gss$marital &lt;- as.character(gss$marital) ggplot(gss, aes(x = marital)) + geom_bar(fill = &quot;blue&quot;,) + scale_x_discrete(labels=c(&quot;1&quot; = &quot;Married&quot;, &quot;2&quot; = &quot;Widowed&quot;,&quot;3&quot; = &quot;Divorced&quot;, &quot;4&quot; = &quot;Separated&quot;, &quot;5&quot; = &quot;Never married&quot;)) + labs(title = &quot;Marital Status of Respondents&quot;, x = &quot;Marital Status&quot;, y = &quot;Frequency&quot;) Barplots are useful when you want to see the relationship between a continuous variable (like partyid) and a categorical variable (like race). But what if we have two continuous variables, like age and how many hours one works a week? We can use scatterplots for that! First, let’s return the data to its original state by ungrouping it. gss &lt;- gss %&gt;% ungroup() Now we can use geom_point, instead of bar, to plot the relationship between age and hours worked last week (hrs1). ggplot(data = gss, aes(x = age, y = hrs1)) + geom_point() ## Warning: Removed 973 rows containing missing values (geom_point). With geom_smooth(), we can add a best fit line to better understand the relationship. ggplot(data = gss, aes(x = age, y = hrs1)) + geom_point() + geom_smooth() Pretty flat, with a bit of a decrease after age 50, and a weird uptick around 100 years old… perhaps that is just noise? Let’s filter out the really old people from the data. It is simple using the filter command. All we have to specify is that we want people of age less than 80, like so. gss_younger &lt;- gss %&gt;% filter(age &lt; 80) geom_smooth() Now we replot the data, except we have to change the name of the data we are using to gss_younger! ggplot(data = gss_younger, aes(x = age, y = hrs1)) + geom_point() + geom_smooth() We can add in another variable using color, if we want. Coloring by sex, for example, reveals remaining discrepencies in workforce participation between males and females, though there is a lot of overlap, and much more than we would have seen twenty years ago. ggplot(data = gss, aes(x = age, y = hrs1, color = sex)) + geom_point() "],["surveys-and-survey-experiments-with-qualtrics.html", "Week 2 Surveys and Survey Experiments with Qualtrics 2.1 Overview 2.2 Creating a Qualtrics account 2.3 Survey options 2.4 A quick survey experiment 2.5 Publish it!", " Week 2 Surveys and Survey Experiments with Qualtrics This section is drafted based on Dr. Mark Hoffman’s previous SOC 10 lab notes. 2.1 Overview In this week, we will go from using data that someone else collected to collecting data of our own using Qualtrics. Qualtrics makes collecting survey data online easy. If you have ever used something like Google Forms, it is, in many respects, quite similar, except that it is much more powerful. As you will see, we can quickly and easily build a survey experiment, administer the experiment, download the results, and import them into R. Let’s get started! 2.2 Creating a Qualtrics account Stanford is kind enough to provide all of us with access to Qualtrics. Click here to get started. Click on the “Set up a Qualtrics account” button and it will tell you to go to https://stanforduniversity.qualtrics.com. Go to that link! There, it will ask if you have a preexisting Qualtrics account or not. Click the button that pertains to you. If you don’t have a preexisting account, it will either prompt you to log-in to your Stanford account via WebAuth, or else it will automatically log-in for you and set up your account if you are already logged in. Now you should be into Qualtrics! Since you haven’t made a project previously, you only have one option: Create a new project (bottom left). Everytime you want to make a new survey you will click this button. We will make an example survey to show you the ropes. It will bring you to a page with a bunch of options for creating a new survey. You can start from these pre-designed surveys if they fall in line with the kind of survey you want to run. There are even pre-built surveys for academics. For learning purposes though, we will start from scratch by clicking “From Scratch–Survey”. Now it asks us to name our project. I called it First Survey. ## Building a survey With a project created, we have been ported to Qualtric’s main user interface, where we can build a survey. Have a look around at all of the different options. Qualtrics is full of tools for creating your ideal survey, but at times that functionality can be a bit overwhelming. We will start simple. Let’s start by learning how to add a question to our survey. Click the menue under “Question type” on the left and select “Text entry”. There are a bunch of options and I recommend experimenting/reading up on all of them. The most essential are Text Entry, Multiple Choice, and Matrix Table. Our first question will use Text Entry. It allows us to ask a question and then have the user put in any value that they like. Multiple Choice on the other hand would let us specify the answers they can provide. For a question like – How old are you? – there are too many potential responses for multiple choice to work well, so text entry will do. We can change the text at the top of the question box to specify the question the respondent should answer: Let’s for example ask “How old are you?” Next let’s make a more complicated multiple choice question using the matrix table. It basically allows you to ask multiple multiple choice questions at once. Let’s start by clicking the blue button “Add new question” and selecting the option “Matrix table”. For example, we can copy a question from the GSS about trust in institutions to see the extent to which our respondents trust different groups, like their family or the government. Cool! Maybe trust of this sort varies by income - we might expect rich people to support institutions more than the poor, for example, because they have historically benefitted from them. If we want to put the income question after age, we can simply drag it upwards so that it comes before the question on trust. We don’t have to come up with all of the questions ourselves - Qualtrics has a question bank that you can use to find pre-written questions. Just click on the “Import from library” button at the bottom of the page. I here import one on education level from the US demographics library. 2.3 Survey options At the bottom, we could add more question blocks (we will skip for now) and edit your end of survey message. You can customize it by scrolling down the options under “Messaging” on the left. Now let’s preview our min-survey by clicking “Preview” button on the top! 2.4 A quick survey experiment To set up a survey experiment, we have to set up a block of questions from which respondents will only get one of the N questions in the block. This selection is randomized, so that we can see how the randomization affects their responses. Here is a quick example. First add a new block. Now we have two blocks. Now add the two different versions of the survey prompt to the block. The last step is to make sure that the questions from this block are sampled randomly for each respondent, and that only one of these questions will be selected. We can achieve this by editing the block’s options. Click the block and select the Question Randomization option one the left. This should pull up the Question Randomization menu. We can choose: no randomization, to randomize the order of questions, or to randomly select N questions from the block to present to the respondent. Select the last optin and set the number to 1 and we will have our first survey experiment. 2.5 Publish it! Now we can either preview again to double check or publish the survey! Once it is published, we can distribute it to respondents. We could do this on Mechanical Turk, as we will learn in a later class, or we can just send it to our friends. To do that click on the “Distributions” tab at the top of the survey. It will ask how you want to distribute your survey. You can do it by email, or by my preferred way, which is to “Get a single reusable link” which you can send to anyone who you want to take your survey. You can also generate a trackable link for each respondent if you want to keep track of who responded. When someone responds to your survey, inside the Data and Analysis pane in Qualtrics, you should see a response! We can export it using the “Export &amp; Import” data button and then clicking Export Data. Export it as a .csv because those files are super easy to load into R. It will download a .zip file to your computer. Once you unzip it, you will be able to load the resulting file into R. First, I would rename it, so that it is easy to input into R. I called the file trial_survey.csv. Drag it to your R project and then you can load it into R using the read.csv() function. Simple! survey &lt;- read.csv(&quot;Data/first_survey.csv&quot;) "],["regression-and-classification.html", "Week 3 Regression and Classification 3.1 Linear Regression 3.2 How linear regression works 3.3 Linear regression using the lm function 3.4 Multiple Regression 3.5 Linear Regression Prediction 3.6 Classification (Logistic Regression)", " Week 3 Regression and Classification This section is drafted based on Dr. Mark Hoffman’s previous SOC 10 lab notes and kassambara’s tutorial on logistic regression with R on STHDA. This week’s lab introduces fundamental machine learning techniques widely used in computational social science to both model and analyze data. In contrast to traditional programming—where explicit instructions are provided at every step—machine learning equips computers with the ability to identify patterns and make decisions based on data. Generally, our objectives in machine learning are threefold: to describe and understand the data, to develop models that test theories about how the data is generated, and to make predictions that generalize to new, similar situations. We begin by exploring Linear Regression, a popular method in the social sciences for modeling relationships between variables. Regression analysis allows us to quantify how changes in one or more predictor variables are associated with changes in a continuous outcome variable. This technique not only provides insights into the underlying patterns of the data but also is useful for predicting continuous outcomes (e.g., household income, movie box office). In addition to regression, we will also cover Classification techniques, with a particular focus on Logistic Regression. Unlike regression, which deals with predicting continuous outcomes, classification methods are designed to predict categorical outcomes. For example, logistic regression models the probability of an observation belonging to a particular category—often a binary outcome—by leveraging a logistic function. The key difference between regression and classification lies in their goals: while regression estimates a numerical value, classification assigns an observation to a discrete class based on learned patterns in the data. 3.1 Linear Regression In the previous two labs, we learned how to download data and visualize patterns between variables. In what follows, we will go beyond data visualization and begin to ask theoretically informed questions and using data, again, from the GSS, to answer those questions. Where before we plotted two variables against each other to see their relationship, linear regression will allow us to quantify their relationship: how much do changes in our explanatory variable lead to changes in the variable we are hoping to explain? Is this relationship statistically significant - that is, does it differ from what we should expect by chance? Linear regression will also allow us to adjust for covariates - variables that may be affecting our primary dependent variable of interest as well as our independent variable. For example, in classic example of confounding, we may see that the number of ice cream cones people eat per year is correlated with the number of sunburns they get and think that ice cream causes sunburns. However, it is obvious that both of these factors will be influenced by how warm of a climate people live in - with people living in warmer climates consuming more ice cream AND getting more sunburns. By controlling for the warmth of the climate, we can adjust for this fact and likely any association we saw between ice cream and sunburns will go away. Figure Source: Parsa Ghaffari. 3.2 How linear regression works You may remember the slope-intercept form of writing the equation of a straight line from algebra. \\[ y = mx + b \\] Here, we can calculate the coordinate y for any x by first multiplying x by the slope of the line, m, and adding the value of the intercept, b, which is where the line intersects with the y axis. Figure Source: tecmath. Linear regression is a strategy for finding the line that best fits the relationship between two variables (later on we can add more variables as control variables). We start with a y variable, also called the outcome or the dependent variable, and an x variable, also called a predictor or the independent variable, and ask what is the slope-intercept equation that most closely approximates their relationship. Given x and y, linear regression therefore involves estimating the slope, m, and intercept, b, of the line. Rarely in real world applications are two variables perfectly related to one another: even the best social science models have error. To reflect this, we update the equation above to: \\[ y = mx + b + ε \\] With ε capturing the error in our predictions. How do we fit a line to x and y? The short of it is that we will start with line (say, a horizontal one) and keep adjusting the slope and intercept of that line to minimize the average distance between the data points and the line itself. Residuals are the difference between the observed values and the values predicted by the line: \\[ residual_i=y_i−\\hat{y}_i \\] Linear regression will seek to minimize the sum of the squared residuals, also known as the sum of squares: \\[ \\text{SumSquares} = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 \\] Figure Source: statisticsfun. 3.3 Linear regression using the lm function Thankfully, as computational social scientists, we won’t have to do this adjustment by hand. The lm function in R uses an algorithm, called gradient descent, to find the linear regression line that best minimizes the sum of squares for us. m1 &lt;- lm(realinc ~ age, data = gss) The first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of real household income as a function of age The second argument specifies that R should look in the gss data frame to find the age and realinc variables. The output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary function. summary(m1) ## ## Call: ## lm(formula = realinc ~ age, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36142 -21953 -8800 11184 88412 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30130.57 1968.22 15.309 &lt;2e-16 *** ## age 74.27 37.87 1.961 0.05 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31160 on 2145 degrees of freedom ## (201 observations deleted due to missingness) ## Multiple R-squared: 0.00179, Adjusted R-squared: 0.001324 ## F-statistic: 3.846 on 1 and 2145 DF, p-value: 0.04999 Let’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of age. With this table, we can write down the least squares regression line for the linear model: \\[ \\hat{y} =30130.57+74.27∗age \\] It also shows whether the coefficients, here, age, have are statistically significant in predicting the outcome, income. Normally, a p-value cut-off of 0.05 is used to determine statistical significance - here, age’s p-value is almost exactly 0.05 (in fact, it is very slightly lower) and is therefore significant. One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, only %.18 of the variability in income is explained by age. What variables might do a better job of explaining income? Let’s try years of education, which is the variable educ in the gss. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between income and education m2 &lt;- lm(realinc ~ educ, data = gss) summary(m2) ## ## Call: ## lm(formula = realinc ~ educ, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -57666 -18112 -6093 10864 97362 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21552 3006 -7.171 1.02e-12 *** ## educ 4006 213 18.809 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28880 on 2149 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.1414, Adjusted R-squared: 0.141 ## F-statistic: 353.8 on 1 and 2149 DF, p-value: &lt; 2.2e-16 \\[ y=−21552+4006∗educ \\] The higher one’s education, the higher one’s income, generally. Specifically, the slope tells us that for every year of education, a person is expected to see an additional income of 4006 dollars. And this result is statistically significant given the small p-value (this is a typical explanation structure for you to explain linear regression results to readers!) Further, the intercept tells us that people with 0 years of education are expect to have an income of -21552 dollars. Of course, this is an extrapolation, produced by the linear regression: an income of negative dollars doesn’t make much sense. Finally, we can see from the regression output that the R2 for education is much higher than age: 11%! Let’s create a scatterplot with the least squares line laid on top. library(ggplot2) ggplot(gss, aes(x = educ, y = realinc)) + geom_point()+ geom_smooth(method=&#39;lm&#39;, formula= y~x) 3.4 Multiple Regression We can build more complex models by adding variables to our model. When we do that, we go from the world of simple linear regression to the more complex world of multiple regression. Here, for example, is a more complex model with both of the variables we examined above. m3 &lt;- lm(realinc ~ age + educ, data = gss) summary(m3) ## ## Call: ## lm(formula = realinc ~ age + educ, data = gss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60809 -19200 -6908 10063 99951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -25708.70 3481.35 -7.385 2.18e-13 *** ## age 83.51 35.11 2.378 0.0175 * ## educ 4012.21 212.96 18.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28870 on 2143 degrees of freedom ## (202 observations deleted due to missingness) ## Multiple R-squared: 0.1436, Adjusted R-squared: 0.1428 ## F-statistic: 179.7 on 2 and 2143 DF, p-value: &lt; 2.2e-16 What do we learn? How does the Multiple R-Squared look compard to previous models? Did we improve our model fit? By including both variables, the coefficients now tell us the effect of each variable, conditional on the other. For example, age and education are correlated in complex ways. Younger generations are slightly more educated than older generations. At the same time, being older gives you more time to achieve an education. Controlling for age allows us to interpret education without worrying that age effects of this sort are driving the relationship between education and income. That said, after age is controlled for, while R2 improves, the coefficient size for education stays roughly the same. Specifically, the slope tells us that controlling for age, for every year of education, a person is expected to see an additional income of 4012.21 dollars. And this result is statistically significant given the small p-value (this is a typical explanation structure for you to explain linear regression results to readers!) You could also add in categorical variables like sex. gss$sex &lt;- as.factor(gss$sex) m4 &lt;- lm(realinc ~ age + educ + sex, data = gss) summary(m4) Call: lm(formula = realinc ~ age + educ + sex, data = gss) Residuals: Min 1Q Median 3Q Max -58696 -18682 -6924 9400 101888 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -23242.75 3539.42 -6.567 6.43e-11 *** age 79.12 35.04 2.258 0.02405 * educ 4025.78 212.41 18.953 &lt; 2e-16 *** sex2 -4473.33 1249.14 -3.581 0.00035 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 28790 on 2142 degrees of freedom (202 observations deleted due to missingness) Multiple R-squared: 0.1487, Adjusted R-squared: 0.1475 F-statistic: 124.7 on 3 and 2142 DF, p-value: &lt; 2.2e-16 Specifically, controlling for age and education, females (vs. males) on average are expected to earn -4473.33 dollars less in income. And this result is statistically significant given the small p-value. However, more variables are not always good! Multicollinearity is an important diagnostic consideration. It occurs when two or more predictor variables in your model are highly correlated with each other, meaning they share redundant information. This redundancy can affect the stability and interpretability of your estimated coefficients. You can check for multicollinearity in your R linear model using VIF: install.packages(&quot;car&quot;) library(car) m4 &lt;- lm(realinc ~ age + educ + sex, data = gss) vif_values &lt;- vif(m4) print(vif_values) age educ sex 1.001432 1.000524 1.001562 The output will show a VIF for each predictor. If any values are high, you may need to consider removing or combining predictors. VIF &gt; 10 is generall indicative of serious multicollinearity that should be addressed. 3.5 Linear Regression Prediction Finally, imagine that you want to predict your own income when you graduate in the coming years using this model. Let’s collect data on what your age and education will be at the time of graduation and use the line to predict your future income. First, we create a data.frame with the necessary data. you &lt;- data.frame(Name = &quot;YOURNAMEHERE&quot;, age = 22, educ = 16) As long as a data.frame has the same variables as those in a model, we can use the predict function to predict the expected outcomes of the respondents in the data.frame using the model. What is your predicted income? predict(m3, you) To get a better prediction, more tailored to you and your background, we would need a more complex model with more variables! 3.6 Classification (Logistic Regression) Logistic regression is used to predict the class (or category) of individuals based on one or multiple predictor variables (x). It is used to model a binary outcome, that is a variable, which can have only two possible values: 0 or 1, yes or no, diseased or non-diseased. The R function glm(), for generalized linear model, can be used to compute logistic regression. You need to specify the option family = binomial, which tells to R that we want to fit logistic regression. mean(gss$realinc,na.rm=T) gss$high_income &lt;- 0 gss$high_income[gss$realinc &gt; 33749.7] &lt;- 1 table(gss$high_income) model &lt;- glm(high_income ~ educ, data = gss, family = binomial) summary(model) Call: glm(formula = high_income ~ educ, family = binomial, data = gss) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -4.67368 0.27149 -17.21 &lt;2e-16 *** educ 0.27891 0.01854 15.04 &lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2954.3 on 2344 degrees of freedom Residual deviance: 2682.4 on 2343 degrees of freedom (3 observations deleted due to missingness) AIC: 2686.4 Number of Fisher Scoring iterations: 4 The model is specified as: \\[ logit(P(high income))= −4.67368 + 0.27891 × educ \\] To convert this log-odds effect into an odds ratio, you exponentiate the coefficient: \\[ exp(0.27891)≈1.322 \\] This means that each additional unit (e.g., year) of education is associated with approximately a 32.2% (1.322-1) increase in the odds of high income. The very low p-value (&lt; 2e-16) suggests that the effect of education on the probability of high income is statistically significant. you &lt;- data.frame(Name = &quot;YOURNAMEHERE&quot;, age = 22, educ = 16) predicted_probability &lt;- predict(model, newdata = you, type = &quot;response&quot;) predicted_probability The output will be the probability that you with the specified education level is in the high_income category. predicted_probability 0.4474074 "],["social-network-analysis.html", "Week 4 Social Network Analysis 4.1 Understanding network data structures in R 4.2 Visualizing network data in R 4.3 Key network measures 4.4 Visualizing Key Measures 4.5 Benchmark our empirical network 4.6 Find local clusters/communities", " Week 4 Social Network Analysis This week our tutorial will cover social network analysis. Social network analysis focuses on patterns of relations between actors and is a key approach to study issues like homopohily and social diffusion. 4.1 Understanding network data structures in R One simple way to represent a graph is to list the edges, which we will refer to as an edge list. For each edge, we just list who that edge is incident on. Edge lists are therefore two column matrices (or dataframe in the language of R) that directly tell the computer which actors are tied for each edge. In a directed graph, the actors in column A are the sources of edges, and the actors in Column B receive the tie. In an undirected graph, order doesn’t matter. Let me use a real social network data as a guiding example for the tutorial. This network is a network of scene coappearances of characters in Victor Hugo’s novel “Les Miserables.” Edge weights denote the number of such occurrences. The data source is D. E. Knuth, “The Stanford GraphBase: A Platform for Combinatorial Computing.” Addison-Wesley, Reading, MA (1993). I converted the raw data format to make it easy for use in R. Let’s load the “lesmis_edges.csv” first, which is the edge list file. edges &lt;- read.csv(&quot;lesmis_edges.csv&quot;) Let’s take a look at this dataframe using function head(): head(edges) source target weight 1 1 2 1 2 1 3 8 3 1 4 10 4 1 5 1 5 1 6 1 6 1 7 1 We can see that there are three columns. The first two columns define the source and the target of the connection. The numerical values in these two columns represent different network nodes (here they represent individual characters in the novel). They do not have to be numerical IDs and we could put in actual character names instead, which is more readable but take up more storage space on our disk. In the case of co-appearence, linkages are not directional. Thus, each pair of node IDs only appear once. For a directed network, each pair of nodes could appear twice (e.g., 1 – &gt; 2 vs. 2 – &gt; 1. In the first case, 1 is the sending node and in the second case, 2 is the sending node). For the edgelist dataframe, we only report node pairs that have links. Thus the third column, which represents the intensity of the connection (i.e., edge weight), is never zero. The “weight” column could have varied values if the network is weighted. The Les Miserables network is weighted that weights represent the number of co-occurence. For some networks, egdes are not weighted (i.e., weight column = 1 for all rows). For such unweighted networks, we could omit the “weight” column. Besides the edgelist data, when conducting social network analysis in R, we often have a second dataframe that stores node-level attributes (though it is not required to have this node-level dataframe to conduct network analysis in R). For the Les Miserables network, we have node-level attributes that store the actual charater names that the nodes correspond to and whether the character is a key character in the novel. nodes &lt;- read.csv(&quot;lesmis_nodes.csv&quot;) Let’s take a look at this dataframe using function head(): head(nodes) id name key_character 1 1 Myriel no 2 2 Napoleon no 3 3 MlleBaptistine no 4 4 MmeMagloire no 5 5 CountessDeLo no 6 6 Geborand no The first column “id” corresponds to the values in the “source” and “target” columns for the edgelist dataframe. For the node dataframe, each node appears once and the columns represent their corresponding attributes. 4.2 Visualizing network data in R Now with the edgelist and the node-level attributes, we are ready to construct a social network using R! Specifically, we will use the “igraph” pacakge in R. If this is your first time using it, again, remeber you need to install it first. install.packages(&quot;igraph&quot;) Then we can activate this package: library(igraph) Then we can construct the network using function graph_from_data_frame(). The first parameter is our edgelist dataframe. The second parameter defines whether or not the network is directed. The third defines node-level attributes: g &lt;- graph_from_data_frame(edges, directed = FALSE, vertices = nodes) We can check some quick statistics for this network. #Number of nodes vcount(g) #Number of edges ecount(g) #Is directed? is_directed(g) #Is weighted? is_weighted(g) #node names V(g)$names #edge weigths E(g)$weight Let’s also quickly plot it! plot( g, main = &quot;Les Misérables Character Co-appearance Network&quot; ) It looks okay but we can for sure make it prettier! set.seed(12) # for reproducible layout plot( g, layout = layout_with_fr(g), vertex.label.cex = 0.7, vertex.size = 6, edge.width = 0.5 * E(g)$weight, # edge widths scale with weight main = &quot;Les Misérables Character Co-appearance Network&quot; ) Wow! Much better! So what we did? layout_with_fr() is a function that forces R to arrange the network using the so-called “Fruchterman and Reingold algorithm”. Essentially, nodes with denser connections are put closer under this algorithm. vertex.label.cex tunes the label size for the node label (character name in this case). vertex.size tunes the size for the node. edge.width adjusts for the width of the edges. There are far more parameters that you can tune! Check out https://igraph.org Now let’s further improve the plot! Recall we have this one additional column called “key_character” for the node dataframe. Let’s highlight the main/key characters with a different color. V(g)$color &lt;- &quot;blue&quot; V(g)$color[V(g)$key_character == &quot;yes&quot;] &lt;- &quot;red&quot; set.seed(12) # for reproducible layout plot( g, layout = layout_with_fr(g), vertex.label.cex = 0.7, vertex.size = 6, vertex.color=V(g)$color, edge.width = 0.5 * E(g)$weight, # edge widths scale with weight main = &quot;Les Misérables Character Co-appearance Network&quot; ) legend( &quot;topleft&quot;, legend = c(&quot;Key character&quot;, &quot;Other character&quot;), pt.bg = c(&quot;red&quot;, &quot;blue&quot;), pch = 21, bty = &quot;n&quot;, cex = 0.8 ) Main characters are indeed more central in the network! Now let’s try to explore the network more to quantitatively describe its structure. 4.3 Key network measures 4.3.1 Degree (“How many friends do I have?”) For an undirected network, the degree of a node n is the count of edges incident on n. For a directed network, a node n can have in-degree (count of inflows) and out-degree (cout of outflows) degree(g) # degree Let’s sort the result. sort(degree(g)) Output: Napoleon CountessDeLo Geborand Champtercier 1 1 1 1 Cravatte Count OldMan Labarre 1 1 1 1 MmeDeR Isabeau Gervais Scaufflaire 1 1 1 1 Boulatruelle Gribier Jondrette MlleVaubois 1 1 1 1 MotherPlutarch Marguerite Perpetue Woman1 1 2 2 2 MotherInnocent MmeBurgon Magnon MmePontmercy 2 2 2 2 BaronessT Child1 Child2 MlleBaptistine 2 2 2 3 MmeMagloire Pontmercy Anzelma Woman2 3 3 3 3 Toussaint Fauchelevent Simplice LtGillenormand 3 4 4 4 Judge Champmathieu Brevet Chenildieu 6 6 6 6 Cochepaille Listolier Fameuil Blacheville 6 7 7 7 Favourite Dahlia Zephine Gillenormand 7 7 7 7 MlleGillenormand Brujon MmeHucheloup Bamatabois 7 7 7 8 Tholomyes Prouvaire Montparnasse Myriel 9 9 9 10 Grantaire Gueulemer Babet Claquesous 10 10 10 10 MmeThenardier Cosette Eponine Mabeuf 11 11 11 11 Combeferre Feuilly Bahorel Joly 11 11 12 12 Courfeyrac Bossuet Fantine Enjolras 13 13 15 15 Thenardier Javert Marius Gavroche 16 17 19 22 Valjean 36 If the network is directed, we can calculate in-degree: degree(g, mode = &quot;in) 4.3.2 Weighted degree (strength) The strength of node n is the sum of edge weights. sort(strength(g)) Napoleon CountessDeLo Geborand Champtercier 1 1 1 1 Cravatte OldMan Labarre MmeDeR 1 1 1 1 Isabeau Gervais Scaufflaire Boulatruelle 1 1 1 1 Jondrette MlleVaubois Count Gribier 1 1 2 2 Magnon MmePontmercy BaronessT Marguerite 2 2 2 3 Perpetue Woman1 Pontmercy MmeBurgon 3 3 3 3 MotherPlutarch MotherInnocent Toussaint Anzelma 3 4 4 5 Woman2 LtGillenormand Child1 Child2 5 5 5 5 MmeHucheloup Simplice Bamatabois Brevet 7 8 11 11 Chenildieu Cochepaille Montparnasse Brujon 11 11 12 13 Fauchelevent Judge Champmathieu Mabeuf 14 14 14 16 Grantaire MlleBaptistine MmeMagloire Eponine 16 17 19 19 Prouvaire Claquesous MlleGillenormand Listolier 19 20 23 24 Fameuil Zephine Blacheville Dahlia 24 24 25 25 Gueulemer Tholomyes Favourite Babet 25 26 26 27 Gillenormand Myriel MmeThenardier Feuilly 29 31 34 38 Bahorel Joly Fantine Javert 39 43 47 47 Gavroche Thenardier Bossuet Cosette 56 61 66 68 Combeferre Courfeyrac Enjolras Marius 68 84 91 104 Valjean 158 4.3.3 Global Clustering Coefficient (GCC) The global clustering coefficient measures the tendency of nodes in a network to cluster together. It quantifies the overall connectivity and cohesion of the network by considering all possible triplets of nodes. In essence, it indicates how much the network resembles a collection of closely connected groups (clusters). transitivity(g, &quot;global&quot;) The output: 0.4989316 Wow the network is very cohesive because GCC is between 0 and 1. 4.3.4 Average path length (APL) Average Path Length (APL) is a key indicator of how interconnected a network is, with shorter APLs generally indicating a more efficient network. APL is calculated by finding the shortest path between all pairs of nodes in the network and then averaging those shortest path lengths. ### average.path.length(g,directed = FALSE) The output is: 4.861244 Essentially, for any two random nodes in the network, it on vargae takes 4.86 steps to reach from one node to the other node. 4.3.5 Assortativity The assortativity coefficient is positive is similar vertices (based on some external property) tend to connect to each, and negative otherwise. Just like the concept of homophoily! assortativity(g, as.factor(V(g)$key_character), directed = FALSE) The output is: -0.08629372 Essentially, main characters engage with supporting characters! 4.3.6 Betweenness centrality Betweenness centrality is a network analysis metric that measures the extent to which a node lies on shortest paths between other nodes in a network. It quantifies the importance of a node in connecting different parts of the network. A node with high betweenness centrality is often a “bridge” or “connector” between various nodes or groups of nodes in the network. sort(betweenness(g, weights = NA)) Napoleon MlleBaptistine MmeMagloire CountessDeLo 0.0000000 0.0000000 0.0000000 0.0000000 Geborand Champtercier Cravatte Count 0.0000000 0.0000000 0.0000000 0.0000000 OldMan Labarre Marguerite MmeDeR 0.0000000 0.0000000 0.0000000 0.0000000 Isabeau Gervais Listolier Fameuil 0.0000000 0.0000000 0.0000000 0.0000000 Blacheville Favourite Dahlia Zephine 0.0000000 0.0000000 0.0000000 0.0000000 Perpetue Scaufflaire Woman1 Judge 0.0000000 0.0000000 0.0000000 0.0000000 Champmathieu Brevet Chenildieu Cochepaille 0.0000000 0.0000000 0.0000000 0.0000000 Boulatruelle Anzelma Woman2 MotherInnocent 0.0000000 0.0000000 0.0000000 0.0000000 Gribier Jondrette MlleVaubois LtGillenormand 0.0000000 0.0000000 0.0000000 0.0000000 BaronessT Prouvaire MotherPlutarch Toussaint 0.0000000 0.0000000 0.0000000 0.0000000 Child1 Child2 MmeHucheloup Grantaire 0.0000000 0.0000000 0.0000000 0.4285714 Magnon MmePontmercy Brujon Combeferre 0.6190476 1.0000000 1.2500000 3.5629149 Feuilly Bahorel Joly Montparnasse 3.5629149 6.2286417 6.2286417 11.0404151 Claquesous Gueulemer Babet Courfeyrac 13.8561420 14.1370943 14.1370943 15.0110352 Pontmercy Bamatabois Simplice Eponine 19.7375000 22.9166667 24.6248408 32.7395194 Gillenormand Cosette MmeBurgon Fauchelevent 57.6002715 67.8193223 75.0000000 75.5000000 Mabeuf MmeThenardier Bossuet Tholomyes 78.8345238 82.6568934 87.6479030 115.7936423 Enjolras MlleGillenormand Javert Thenardier 121.2770669 135.6569444 154.8449450 213.4684805 Fantine Marius Gavroche Myriel 369.4869418 376.2925926 470.5706319 504.0000000 Valjean 1624.4688004 There are many other types of centralities (eigenvector centrality, closeness centrality, degree centrality) that measure the importance of different nodes. 4.4 Visualizing Key Measures Let’s try to visualize some of the key measures on the network plot! For example, we can make node size proportional to degree: V(g)$size &lt;- degree(g) set.seed(12) # for reproducible layout plot( g, layout = layout_with_fr(g), vertex.label.cex = 0.7, vertex.size = V(g)$size / 2, edge.width = 0.5 * E(g)$weight, # edge widths scale with weight main = &quot;Les Misérables Character Co-appearance Network&quot; ) For example, we can make node size proportional to betweenness centrality measure: V(g)$size &lt;- betweenness(g, weights = NA) set.seed(12) # for reproducible layout plot( g, layout = layout_with_fr(g), vertex.label.cex = 0.7, vertex.size = V(g)$size / 100, edge.width = 0.5 * E(g)$weight, # edge widths scale with weight main = &quot;Les Misérables Character Co-appearance Network&quot; ) 4.5 Benchmark our empirical network I just introduced the measure of GCC and APL. You may wonder, for our empirical network, is the GCC/APL high or low? We often need to benchmark it to a random graph to answer this question. A random graph (also known as Erdős–Rényi random graph) preseves the same number of nodes and edges for an empirical network. However, the edges are randomly conencted for the random network. set.seed(123) n &lt;- vcount(g) m &lt;- ecount(g) g_r &lt;- sample_gnm(n, m, directed = FALSE, loops = FALSE) gcc_rand &lt;- transitivity(g_r, type = &quot;global&quot;) apl_rand &lt;- average.path.length(g_r, directed = FALSE) The GCC for the random network is only 0.0745003 and the APL is 2.471634. What is the implication? The random network is less locally clustered but more globally connected! set.seed(12) # for reproducible layout plot( g_r, layout = layout_with_fr(g_r), vertex.label = NA, vertex.size = 5, edge.width = 1, main = &quot;Random Network&quot; ) Can we have both high GCC and low APL? Yes! The small-world network! 4.6 Find local clusters/communities There are multiple community detection methods. louvain is a popular one and it works for weighted network. Community detection (weighted Louvain) com &lt;- cluster_louvain(g, weights = E(g)$weight) # &quot;Number of communities found&quot; length(com) V(g)$community &lt;- membership(com) V(g)$color &lt;- V(g)$community set.seed(12) plot( g, layout = layout_with_fr(g), vertex.label.cex = 0.7, vertex.label.color= &quot;black&quot;, vertex.size = 5, vertex.color= V(g)$color, edge.width = 0.5 * E(g)$weight, main = &quot;Les Misérables – Louvain Communities&quot; ) 4.6.1 Other Resources Social network analysis is a powerful tool and there are much more could be done to our data! Check out https://r.igraph.org/ for more network analysis techniques using R. If you are interested in social network datasets, check out https://icon.colorado.edu/networks Also check out this awesome interactive social network page on “Six Degrees of Francis Bacon”: http://sixdegreesoffrancisbacon.com/?ids=10000473&amp;min_confidence=60&amp;type=network "],["collecting-data-online.html", "Week 5 Collecting Data Online 5.1 Scraping the web 5.2 Google News API", " Week 5 Collecting Data Online This section is drafted based on Dr. Mark Hoffman’s previous SOC 10 lab notes. Conducting research via webpage scrapping and API call is likely under the regulation of University IRB board. Please consult Stanford IRB office for detail. In previous tutorials, we learned how to download datasets online or collect them ourselves using survey software like Qualtrics and to load them into R for analysis. A lot of computational social science data, however, doesn’t come in such easily downloadable form. This is for a couple reasons. For one, companies might provide their data for viewing on a webpage rather than in a .csv or .dta file. Wikipedia, for example, contains thousands and thousands of data tables, concerning topics from GDP per capita by country to the number and types of awards won by Kanye West. These data are publicly available in the form of tables on their various webpages; but there is no single place where you can download all of them. We therefore will need to learn how to grab this data off their webpages using something called web scraping or crawling in R. This is, in part, what Google uses to index the content of websites and offer them to you following your searches. Alternatively, the amount of data that a website provides might just be too large or too heterogenous to reasonably put into a single, downloadable file or else webistes might have privacy and intellectual property concerns, which preclude them from making all of their data easily available to everyone. In such situations, a webiste or organization might provide a tool for accessing their data in an approved fashion, often referred to as an application programming interface (API). In tutorial, we will learn the aforementioned skills for collecting online data - web scraping and using APIs. Each website and API is different, so the tutorials presented here might not always apply precisely to every use case, but the basic principles should help get you started collecting digital trace data. 5.1 Scraping the web In the first portion of this tutorial, we will cover web scraping. Web scraping involves pulling the underlying code – HTML, CSS, or Javascript – of website and interpreting or collecting information embedded in that code. When you visit a website, your browser reads the HTML, CSS, and Javascript and through interpreting them, learns how to display that website. HTML defines the meaning and structure of web content, while CSS (Cascading Style Sheets) and Javascript in turn define how that content appears and behaves, respectively. One of the best packages in R for webscraping is rvest. In particular, it focuses on pulling data from html web pages and has a number of functions for doing so. You could build a program which grabs the HTML from websites and searches through it for information, but it would be very difficult. rvest has done the difficult work for you. Let’s begin by installing rvest. install.packages(&quot;rvest&quot;) Now we can load it into R. Let’s load in tidyverse too. library(rvest) library(tidyverse) Great! If you want to read more about rvest beyond what is covered here, check out its reference manual online: https://cran.r-project.org/web/packages/rvest/rvest.pdf The core function in rvest for grabbing the html data from a website is html(). We will use it to grab data from Wikipedia about the GDP per person employed. We could grab any webpage using this function and it is worth exploring on your own time. webpage &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_person_employed&quot;) webpage You should be able to see the below output: {html_document} &lt;html class=&quot;client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; chars ... [2] &lt;body class=&quot;skin--responsive skin-vector skin-vector-search-vue ... Cool! With the HTML in our R environment, we can now use other functions to extract information from it. How do we do that? Well first, we have to know what function to use. There are quite a few - for example, html_attr extracts text or tag names, html_nodes extracts sections or divisions of the html file by name or type, html_table extracts tables inside of sections, and html_text extracts text. Once we know the function that we need to use, then we have to figure out what we want to pull from the html. Go to our URL: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_person_employed Find the table to extract. Right-click the table -&gt; click Inspect A pop-up menu will show and you will need to select the table element Right-click the table element -&gt; Copy -&gt; Copy Xpath webpage_table_html &lt;-html_nodes(webpage, xpath=&#39;/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table/tbody/tr[2]/td[1]/table&#39;) webpage_table_html Output: {xml_nodeset (1)} [1] &lt;table class=&quot;wikitable sortable&quot; style=&quot;margin-left:auto;margin- ... The problem is that the table is still in html format - thankfully, rvest has a function, html_table, which converts such information into an R data.frame. It saves each table in the html as a data.frame inside of a single list, so we will have to index the list we want. In this case, there is only one table on the page so we will index the first item of the list. gdp_info &lt;- html_table(webpage_table_html, fill = T, trim = T) class(gdp_info) Output: [1] &quot;list&quot; gdp_info &lt;- gdp_info[[1]] There isn’t much we can do with a single column of data like this. So what if we scraped data about countries from another Wikipedia page and merged it to this one? For example, we could evaluate if GDP per hour worked (which implicitly adjusts for country size and captures hourly returns to labor) is correlated with how a country performed at the 2016 Olympic games. First let’s grab the info just like we did before. # Grab the html olympics_webpage &lt;- read_html(&quot;https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table&quot;) # Extract the table olympics_webpage_table_html &lt;-html_nodes(olympics_webpage, xpath=&#39;/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[3]&#39;) # Convert the table to a data.frame medals_info &lt;- html_table(olympics_webpage_table_html, fill = T, trim = T) medals_info &lt;- medals_info[[1]] Then let’s inspect the data. Does it look like it can be easily merged with our GDP data? Perhaps, but there is one problem. Some countries (e.g., United States and France ) have special symbols after their names. # also need tidyverse pakacge library(stringr) medals_info &lt;- medals_info %&gt;% mutate( NOC = NOC %&gt;% str_remove_all(&quot;[^[:alpha:][:space:]]&quot;) %&gt;% # keep only letters &amp; spaces str_to_lower() %&gt;% # convert to lowercase str_squish() # trim / collapse spaces ) head(medals_info) Output: # A tibble: 6 × 6 Rank NOC Gold Silver Bronze Total &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 united states 40 44 42 126 2 2 china 40 27 24 91 3 3 japan 20 12 13 45 4 4 australia 18 19 16 53 5 5 france 16 26 22 64 6 6 netherlands 15 7 12 34 Let’s do the same for our GDP data gdp_info &lt;- gdp_info %&gt;% mutate( Country = Country %&gt;% str_remove_all(&quot;[^[:alpha:][:space:]]&quot;) %&gt;% # keep only letters &amp; spaces str_to_lower() %&gt;% # convert to lowercase str_squish() # trim / collapse spaces ) Now that both datasets have the same way of writing country names, we can merge the data by Country. merged_df &lt;- merge(gdp_info, medals_info, by.x = &quot;Country&quot;, by.y=&quot;NOC&quot;) Take a look at the size of merged_df on your Environment panel. What you notice? It only contains 82 rows. Why? This is because only 82 countries have both GDP and medal infromation. Alternatively, if you want to keep all countries which appeared in the GDP table, you can merged_df_gdp &lt;- merge(gdp_info, medals_info, by.x = &quot;Country&quot;, by.y=&quot;NOC&quot;,all.x=TRUE) And we can plot results using ggplot, just like we did in our first R lesson! First, let’s select the columns we want using tidyverse. merged_df &lt;- merged_df %&gt;% select(Country,`GDP per personemployed(2017 Intl. $)`,Total,Gold) %&gt;% rename(Country = Country,GDP_22 = `GDP per personemployed(2017 Intl. $)`,Total_Medal_24 = Total, Gold_24 = Gold) head(merged_df) Output: Country GDP_22 Total_Medal_24 Gold_24 1 albania 34,018 2 0 2 algeria 45,619 3 2 3 argentina 52,012 3 1 4 armenia 34,869 4 0 5 australia 97,250 53 18 6 austria 112,589 5 2 Let’s plot now! ggplot(merged_df, aes(x = GDP_22, y = Gold_24, label = Country)) + geom_text() + geom_smooth(method=&#39;lm&#39;, formula= y~x) This plot is terrible! What is up with the x-axis? It doesn’t seem to be detecting either axis as numbers. Let’s convert them using as.numeric so that R doesn’t get confused again. There is a problem though.. the GDP values have commas in them. R won’t recognize them as potential numbers and will return NA when we run as.numeric. We can use the gsub function to substitute things in a string for something else. Let’s use it to remove the commas by substituting commas with empty strings. merged_df$GDP_22 &lt;- gsub(&quot;,&quot;, &quot;&quot;, merged_df$GDP_22) merged_df$GDP_22 &lt;- as.numeric(merged_df$GDP_22) Let’s try again! ggplot(merged_df, aes(x = GDP_22, y = Gold_24, label = Country)) + geom_text() + geom_smooth(method=&#39;lm&#39;, formula= y~x) + xlab(&quot;GDP per person employed&quot;) What do you see? Seems like there is a positive correlation between the two! Let’s also fit a simple linear regression on it! Model1 &lt;- lm(Gold_24 ~ GDP_22, data = merged_df) summary(Model1) Output: Call: lm(formula = Gold_24 ~ GDP_22, data = merged_df) Residuals: Min 1Q Median 3Q Max -7.992 -2.786 -1.710 -0.210 37.568 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.846e-01 1.401e+00 0.703 0.4843 GDP_22 4.235e-05 1.872e-05 2.262 0.0264 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 6.963 on 80 degrees of freedom Multiple R-squared: 0.06012, Adjusted R-squared: 0.04837 F-statistic: 5.117 on 1 and 80 DF, p-value: 0.0264 The explanatory power of GDP per capita is weak! What other factors you can think of that contribute to the number of gold medals? What if we exclude China and United States? merged_df_filtered &lt;- merged_df %&gt;% filter(Country != &quot;china&quot; &amp; Country != &quot;united states&quot;) ggplot(merged_df_filtered, aes(x = GDP_22, y = Gold_24, label = Country)) + geom_text() + geom_smooth(method=&#39;lm&#39;, formula= y~x) + xlab(&quot;GDP per person employed&quot;) The pattern is still far from linear. 5.2 Google News API This section is drafted based on https://paulcbauer.github.io/apis_for_social_scientists_a_review/google-news-api.html. Google News API allows you to extract news headlines and contents from various global news sources. install.packages(&#39;httr&#39;) library(httr) 5.2.1 Prerequistes You need an API key, which can be requested via https://newsapi.org/register. Note that there are a set of limitations for the free version. You can purchase a paid version for better service. But a free version is good enough for this lab’s deomonstration! 5.2.2 Get started The API has three main endpoints: https://newsapi.org/v2/everything?, documented at https://newsapi.org/docs/endpoints/everything https://newsapi.org/v2/top-headlines/sources?, documented at https://newsapi.org/docs/endpoints/sources https://newsapi.org/v2/top-headlines?, documented at https://newsapi.org/docs/endpoints/top-headlines Let’s try to use the everything endpoint to search for keyword “Canada” endpoint_url &lt;- &quot;https://newsapi.org/v2/everything?&quot; my_query &lt;- &quot;canada&quot; my_start_date &lt;- Sys.Date() - 7 Sys.setenv(NEWS_API_KEY = &quot;change to your actual api key here!&quot;) #change to your actual API key my_api_key &lt;- Sys.getenv(&quot;NEWS_API_KEY&quot;) params &lt;- list( &quot;q&quot; = my_query, &quot;from&quot; = my_start_date, &quot;language&quot; = &quot;en&quot;, &quot;sortBy&quot; = &quot;publishedAt&quot;) news &lt;- httr::GET(url = endpoint_url, httr::add_headers(Authorization = my_api_key), query = params) result &lt;- httr::content(news) Let’s see the articles we get! second_article &lt;- result$articles[[2]] second_article$title second_article$url second_article$source second_article$description &gt; second_article$title [1] &quot;Amazon launches its first internet satellites to compete against SpaceX’s Starlinks&quot; &gt; second_article$url [1] &quot;https://financialpost.com/pmn/amazon-launches-its-first-internet-satellites-to-compete-against-spacexs-starlinks&quot; &gt; second_article$source $id [1] &quot;financial-post&quot; $name [1] &quot;Financial Post&quot; &gt; second_article$description [1] &quot;CAPE CANAVERAL, Fla. (AP) — Amazon’s first batch of internet satellites rocketed into orbit Monday, the latest entry in the mega constellation market currently dominated by SpaceX’s thousands of Starlinks. The United Launch Alliance’s Atlas V rocket carried u…&quot; "],["text-analysis.html", "Week 6 Text Analysis 6.1 Sentiment Analysis 6.2 Topic Modeling 6.3 Word Embeddings", " Week 6 Text Analysis This section is inspired by https://m-clark.github.io/text-analysis-with-R/sentiment-analysis.html. Text data are now everywhere: tweets and forum posts, press releases and policy documents, interview transcripts and open-ended survey answers. Turning that unstructured torrent into systematic evidence is quickly becoming a core skill for social scientists, data analysts, and anyone who needs to understand language at scale. This hands-on R tutorial walks you through three foundational techniques—sentiment analysis, topic modeling, and word embeddings—that together cover the full spectrum from simple lexicon-based scoring to advanced distributional semantics. 6.1 Sentiment Analysis Sentiment analysis is often the most intuitive entry point to text analytics. Its goal is to quantify the emotional tone of a corpus. Although a human reader can judge sentiment in a single sentence, automated methods are indispensable when the dataset grows to hundreds of thousands or even millions of sentences and documents. The tidytext package in R streamlines this process. Let’s start by installing this pacakge. install.packages(&quot;tidytext&quot;) library(tidytext) A common tidy-text sentiment method treats a document’s sentiment as the sum of the sentiments of its individual words. We can use existing lexicons to map individual words with sentiment. Lexicons available in tidytext: AFINN – assigns each word an integer score from –5 (very negative) to +5 (very positive). bing – labels words simply as positive or negative. nrc – labels words as positive or negative and in eight basic emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust). All three lexicons are unigram-based and cover thousands of English words, making them general-purpose tools for quick sentiment tagging within the tidy ecosystem. Let’s take a look at the “bing” lexicon. get_sentiments(&quot;bing&quot;) # A tibble: 6,786 × 2 word sentiment &lt;chr&gt; &lt;chr&gt; 1 2-faces negative 2 abnormal negative 3 abolish negative 4 abominable negative 5 abominably negative 6 abominate negative 7 abomination negative 8 abort negative 9 aborted negative 10 aborts negative # ℹ 6,776 more rows # ℹ Use `print(n = ...)` to see more rows Let’s store the “bing” lexicon into a dataframe and use it to assess the sentiment of the novel “The Wonderful Wizard of Oz”. bing_sent &lt;- get_sentiments(&quot;bing&quot;) Lexicon gives us a dictionary/reference to label the sentiments of the words that are under evaluation. But how exactly should we conduct sentiment analysis for a text? Below visualizes the workflow of conducting sentiment analysis using tidytext Figure Source. Let’s start by importing the text of the novel “The Wonderful Wizard of Oz”. We can access the text via the Project Gutenberg (https://www.gutenberg.org), a library of over 75,000 free eBooks. Let’s install the R packakge for Project Gutenberg and retrieve the text of the novel “The Wonderful Wizard of Oz”. install.packages(&quot;gutenbergr&quot;) library(gutenbergr) book_oz = gutenberg_works(title == &quot;The Wonderful Wizard of Oz&quot;) Let’s take a look of the book_oz object we just created. book_oz # A tibble: 1 × 8 gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf rights has_text &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; 1 43936 The Wonderful Wizar… Baum,… 42 en &quot;&quot; Publi… TRUE We can donwload the text by going to the website and using our gutenberg_id. text_oz = gutenberg_download(book_oz$gutenberg_id) Let’s take a look at text_oz and notice that there are still things to be cleaned. We first slice off the initial parts we don’t want like title, author etc. Then we get rid of other tidbits that would interfere, using a little regex as well to aid the process. library(tidyverse) text_oz_filtered = text_oz %&gt;% slice(-(1:20)) %&gt;% #Drops the first 20 rows (negative slice = “all but”). These rows are usually Gutenberg boiler-plate (title page, licensing notes, table of contents). filter(!text==str_to_upper(text), # Eliminates rows whose entire string is in ALL-CAPS. In a Shakespeare script those are mostly block headers—THE PROLOGUE, ENTER HORATIO, etc.—which carry no narrative sentiment. !text==str_to_title(text), # Eliminates rows whose string equals its Title-Case version. !str_detect(text, pattern=&#39;^(Scene|SCENE)|^(Act|ACT)|^\\\\[&#39;)) %&gt;% select(-gutenberg_id) %&gt;% #Drops the unneeded gutenberg_id column to declutter the data frame. unnest_tokens(sentence, input=text, token=&#39;sentences&#39;) %&gt;% #Splits each surviving line into individual sentences (tidytext tokenization), giving one row per sentence—ideal granularity for sentiment scoring. mutate(sentenceID = 1:n()) #Adds a running sentenceID so every sentence has a unique key for joins, plotting, or ordering later on. In addition, we can remove stopwords like a, an, the etc., and tidytext comes with a stop_words data frame. head(stop_words$word) &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; &quot;above&quot; &quot;according&quot; Let’s now parse the sentence-level document into words and remove stop_words using anti_join. text_oz_filtered = text_oz_filtered %&gt;% unnest_tokens(output=word, input=sentence, token=&#39;words&#39;) %&gt;% anti_join(stop_words) Let’s take a look at the the most frequent words appeared in the novel. text_oz_filtered %&gt;% count(word) %&gt;% arrange(desc(n)) # A tibble: 2,508 × 2 word n &lt;chr&gt; &lt;int&gt; 1 dorothy 343 2 scarecrow 215 3 woodman 172 4 lion 171 5 oz 161 6 tin 139 7 witch 123 8 green 105 9 girl 93 10 head 90 # ℹ 2,498 more rows # ℹ Use `print(n = ...)` to see more rows We can then merge the word-level data with our Bing lexicon to map words with sentiments. oz_sentiment = text_oz_filtered %&gt;% inner_join(bing_sent) oz_sentiment # A tibble: 1,943 × 3 sentenceID word sentiment &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 2 healthy positive 2 2 wholesome positive 3 3 love positive 4 3 fantastic positive 5 3 marvelous positive 6 4 unreal positive 7 6 happiness positive 8 6 childish negative 9 11 horrible negative 10 12 fearsome negative # ℹ 1,933 more rows # ℹ Use `print(n = ...)` to see more rows We can now aggregate the sentiments to the sentence level. Let’s count positive vs. negative words in each sentence since we assign binary sentiments to words. sentiment_counts &lt;- oz_sentiment %&gt;% count(sentenceID, sentiment) %&gt;% # number of words by sentiment per sentence pivot_wider(names_from = sentiment, # … → wide form: one row per sentence values_from = n, values_fill = 0) # sentences w/ no pos/neg get 0 Let’s get an assessment on the sentence-level sentiment by taking the difference between the count of positive words and the count of negative words. sentiment_counts$diff &lt;- sentiment_counts$positive - sentiment_counts$negative ggplot(sentiment_counts, aes(x = sentenceID, y = diff)) + geom_line(size = 0.8) + labs(title = &quot;Sentence-level Sentiment in *The Wonderful Wizard of Oz*&quot;, x = &quot;Sentence order in text&quot;, y = &quot;Sentence-level Sentiment&quot;) Let’s smooth the plot a bit by grouping every 50 sentences together. sentiment_counts %&gt;% mutate(window = sentenceID %/% 50) %&gt;% # 50-sentence blocks group_by(window) %&gt;% summarise(diff = sum(positive) - sum(negative)) %&gt;% ggplot(aes(window, diff)) + geom_line() 6.2 Topic Modeling Topic modeling is an unsupervised machine-learning technique that scans a collection of documents and groups words that frequently appear together into latent “topics.” Each topic represents a coherent theme—such as “injury reports,” “transfer rumors,” or “playoff predictions”—and every article is scored on how strongly it exhibits each theme. In this tutorial, I will run a topic model on the latest 100 sports-news articles pulled via a News API. It would quantify how much attention different sports, teams, or issues are receiving. In short, it turns a raw stream of headlines into a structured map of the current sports-news agenda. Let’s go back to the News API to retrieve the latest 100 articles from ESPN. library(httr) # talk to web APIs library(jsonlite) # parse JSON responses endpoint_url &lt;- &quot;https://newsapi.org/v2/everything&quot; Sys.setenv(NEWS_API_KEY = &quot;insert your api here&quot;) my_api_key &lt;- Sys.getenv(&quot;NEWS_API_KEY&quot;) params &lt;- list( domains = &quot;espn.com&quot;, # only ESPN language = &quot;en&quot;, pageSize = 100, # max per call sortBy = &quot;publishedAt&quot; ) resp &lt;- GET( url = endpoint_url, query = params, add_headers(Authorization = my_api_key) ) ## Parse JSON → tibble with title + description articles_df &lt;- content(resp, as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% fromJSON(flatten = TRUE) %&gt;% pluck(&quot;articles&quot;) %&gt;% as_tibble() %&gt;% select(title, description) # keep only what we need Let’s take a look at articles_df head(articles_df) # A tibble: 6 × 2 title description &lt;chr&gt; &lt;chr&gt; 1 Hamilton Miami radio drama borne from Ferrari&#39;s lack of pace &quot;Lewis Hamilton… 2 Reds rookie Callihan injures arm on sliding catch &quot;With two outs … 3 Dodgers&#39; Teoscar Hernández (hamstring) exits &quot;Dodgers outfie… 4 Mets&#39; Winker out 6-8 weeks, Minter&#39;s season over &quot;New York Mets … 5 Man takes 1st steps after 21-foot fall at PNC Park &quot;The 20-year-ol… 6 Follow live: Braves working on no-hitter vs. Reds &quot;Live coverage … Let’s further clean the corpus corpus &lt;- articles_df %&gt;% filter(!is.na(description)) %&gt;% # drop blanks mutate(doc_id = row_number()) tokens &lt;- corpus %&gt;% unnest_tokens(word, description) %&gt;% # one row = one word anti_join(stop_words, by = &quot;word&quot;) %&gt;% # remove stop-words filter(str_detect(word, &quot;[a-z]&quot;)) # drop numbers etc. Let’s create a Document-Term Matrix (articles × words). library(tm) dtm &lt;- tokens %&gt;% count(doc_id, word) %&gt;% cast_dtm(document = doc_id, term = word, value = n) Let’s fit a topic model with 5 topics! library(topicmodels) set.seed(42) k &lt;- 5 lda_model &lt;- LDA(dtm, k = k, control = list(seed = 42)) top_terms &lt;- tidy(lda_model, matrix = &quot;beta&quot;) %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% # 10 “strongest” words ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 104 × 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 sunday 0.0224 2 1 round 0.0187 3 1 left 0.0112 4 1 monday 0.0112 5 1 race 0.0112 6 1 team 0.0112 7 1 win 0.0112 8 1 season 0.0112 9 1 title 0.0112 10 1 lewis 0.00746 top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() + labs(title = &quot;Top-10 keywords in each topic (ESPN, latest 100)&quot;, x = NULL, y = &quot;β (topic-word weight)&quot;) What we see based on the top keywords in each topic: • Topic 1: post-game wrap-ups / championships • Topic 3: NFL &amp; MLB rumours • Topic 5: Liverpool / soccer • Topic 2: generic words (time, monday, sources) → weak • Topic 4: catch-all rankings / lists → broad How can we improve? Let’s drop obviously non-semantic tokens first, such as words like monday, time, espn are schedule/meta terms, not content. custom_stop &lt;- c(stop_words$word, &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;, &quot;friday&quot;, &quot;saturday&quot;, &quot;sunday&quot;, &quot;espn&quot;, &quot;time&quot;, &quot;sources&quot;) tokens &lt;- tokens %&gt;% filter(!word %in% custom_stop) And let’s try a smaller topic size. let’s try 4. Let’s recreate the Document-Term Matrix (articles × words). dtm &lt;- tokens %&gt;% count(doc_id, word) %&gt;% cast_dtm(document = doc_id, term = word, value = n) Let’s fit a topic model with 4 topics! library(topicmodels) set.seed(42) k &lt;- 4 lda_model &lt;- LDA(dtm, k = k, control = list(seed = 42)) top_terms &lt;- tidy(lda_model, matrix = &quot;beta&quot;) %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% # 10 “strongest” words ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 66 × 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 left 0.0231 2 1 top 0.00990 3 1 star 0.00990 4 1 day 0.00990 5 1 list 0.00990 6 1 lewis 0.00660 7 1 inning 0.00660 8 1 play 0.00660 9 1 game 0.00660 10 1 hamstring 0.00660 # ℹ 56 more rows # ℹ Use `print(n = ...)` to see more rows top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() + labs(title = &quot;Top-10 keywords in each topic (ESPN, latest 100)&quot;, x = NULL, y = &quot;β (topic-word weight)&quot;) What we observe now? Topic 1 reads like a “roster moves / injury-updates” grab-bag: headlines about a player left off a squad list, someone placed on retroactive injured list, betting odds if a star “might move”. Still littered with generic tokens (top, day, list). Topic 2 a broad competition &amp; tournament theme (World Cup, playoff series, round of X). Topic 3 is very clear: Liverpool FC / Premier-League storylines, focused on Trent Alexander-Arnold and title talk. Topic 4 feels like rankings / power-list / draft coverage; many “Top 10 Teams” or “Live Draft” style blurbs. Still noisy (live, york, espn’s). Let’s now turn to topic proportions per document (γ matrix). Essentially, we measure the topic identity for each of the 100 news articles. doc_topics &lt;- tidy(lda_model, matrix = &quot;gamma&quot;) doc_topics # A tibble: 400 × 3 document topic gamma &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.993 2 2 1 0.997 3 3 1 0.996 4 4 1 0.996 5 5 1 0.00157 6 6 1 0.00146 7 7 1 0.00170 8 8 1 0.00114 9 9 1 0.00108 10 10 1 0.00128 # ℹ 390 more rows # ℹ Use `print(n = ...)` to see more rows Let’s now plot average prominence of each topic across the corpus. doc_topics %&gt;% group_by(topic) %&gt;% summarise(mean_gamma = mean(gamma)) %&gt;% ggplot(aes(factor(topic), mean_gamma, fill = factor(topic))) + geom_col(show.legend = FALSE) + labs(title = &quot;Share of attention each topic gets (all 100 stories)&quot;, x = &quot;Topic&quot;, y = &quot;Mean γ (document-topic proportion)&quot;) We can see that topic 4 (on draft and rankings) is most salient across the articles. In this section, we see the usefulness of topic modeling and also see its weakness. First, we need to pre-specify K. That is, we must choose the number of topics in advance. In addition, high-frequency function words can dominate a topic if not carefully filtered/weighted. Finally,some learned topics are statistical artifacts (mixtures of unrelated high-probability words). 6.3 Word Embeddings We have encountered the concept of word embeddings multiple times in the readings. It is now the time to actually see how it works! Essentially, a word embedding instead gives every word a vector—just a short list of numbers, e.g. \\[ lion → [-0.11, 0.87, 0.02, …, 0.35] \\\\ coward → [ 0.64, -0.57, …, -0.12] \\] Words that appear in similar contexts (“The lion showed great courage”) will get vectors that sit near each other in this multi-dimensional space. We can then: 1. measure semantic similarity (cosine distance), 3. add &amp; subtract meanings (king – man + woman ≈ queen), 3. feed the vectors into regressions. Let’s see how to construct word embeddings from text! We already built text_oz_filtered in the sentiment section. It is a data-frame where each row is a word (token). library(text2vec) # Framework for GloVe / word2vec in R tokens &lt;- text_oz_filtered$word # grab the column as a character vector length(tokens) # sanity-check: ~25 000 tokens 12322 Now we need to decide which words are worth learning vectors for. tokens_list &lt;- list(tokens) it &lt;- itoken(tokens_list, progressbar = FALSE) # text2vec “iterator” over tokens vocab &lt;- create_vocabulary(it) %&gt;% prune_vocabulary(term_count_min = 5) # build + prune Each row of vocab now lists a word (“term”) and how many times it appeared. Low-frequency words are thrown away because the model has too little information to pin down their meaning. Let’s now build a Term–Co-Occurrence Matrix (TCM). Why? GloVe does not look at documents; it looks at windows of neighbouring words. For every pair of words, it counts how often they show up within, say, 5 words of each other. vectorizer &lt;- vocab_vectorizer(vocab) # tells text2vec how to convert tokens → IDs tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L) # ±5-word window around a focus term dim(tcm) # (vocabulary size) × (vocabulary size) sparse matrix Let’s check what is inside lion Dorothy heart ... lion 0 27 12 Dorothy 27 0 15 heart 12 15 0 ... Inside the matrix, a value of 27 means “lion” appeared within five words of “Dorothy” 27 times. The matrix is sparse and stored efficiently—most pairs never co-occur. Now we can run GloVe model, a popular word embedding method! Parameter Intuition rank Output dimensions. More ≈ richer detail but needs bigger data. x_max Caps the influence of very frequent pairs; 10 is usual. n_iter Training passes over the TCM. Watch the loss metric—stop early if it plateaus. set.seed(42) # reproducible experiments glove &lt;- GlobalVectors$new(rank = 50, # 50 dims ≈ sweet spot for small corpora x_max = 10) # how strongly to down-weight rare pairs word_vectors_main &lt;- glove$fit_transform(tcm, n_iter = 15, # training epochs convergence_tol = 1e-3) # stop if improvement &lt; ε # GloVe learns two sets of vectors (main + context). Sum them: word_vectors &lt;- word_vectors_main + t(glove$components) dim(word_vectors) # (vocabulary size) × 50 Now let’s find nearest neighbours sim_words &lt;- function(term, n = 3) { sims &lt;- sim2(word_vectors, word_vectors[term, , drop = FALSE], method = &quot;cosine&quot;, norm = &quot;l2&quot;) sort(sims[,1], decreasing = TRUE)[2:(n+1)] # drop self-similarity } sim_words(&quot;dorothy&quot;) scarecrow oz replied 0.7104297 0.6610949 0.6609231 Key take-aways 1. Word embeddings let you turn raw text into numeric features that “know” something about meaning. 2. Even a tiny corpus like The Wonderful Wizard of Oz gives vectors that make intuitive sense. 3. Once vectors exist, you can visualise them, cluster them, feed them into any machine-learning or statistical model—exactly as you would with height, age, or income. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
